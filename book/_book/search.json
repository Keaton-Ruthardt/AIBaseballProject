[
  {
    "objectID": "week2.html",
    "href": "week2.html",
    "title": "3  Week 2: Vision Pipeline and Feature Engineering",
    "section": "",
    "text": "3.1 Overview\nWeek 2 marked a major technical milestone with the development of our core computer vision pipeline and the engineering of critical spatial features. The team successfully transformed raw video into structured, model-ready data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2: Vision Pipeline and Feature Engineering</span>"
    ]
  },
  {
    "objectID": "week2.html#team-contributions",
    "href": "week2.html#team-contributions",
    "title": "3  Week 2: Vision Pipeline and Feature Engineering",
    "section": "3.2 Team Contributions",
    "text": "3.2 Team Contributions\n\nDiego Mendoza: Documentation & Feature Playbook\nObjective: Document all features and establish data standards\nMLB-Provided Features:\n\nrunner_base: Base occupied at pitch (1B/2B/3B)\nhit_distance: Ball travel distance (feet)\nbearing: Horizontal angle from home to landing/catch (degrees)\nlaunch_direction: Pull/center/opposite field orientation\nexit_speed & exit_speed_squared: Ball speed off bat (MPH and squared)\nhangtime: Flight duration (seconds)\nlaunch_angle: Vertical angle at contact (degrees)\n\nTeam-Computed Features:\n\noutfield_spread_event: Spacing between LF/CF/RF\nfielder_pos: Which outfielder records the catch\n\n\n\nJoshua Cano: Frame Extraction Pipeline\nObjective: Build reliable OpenCV pipeline for analysis-ready frames\nTechnical Implementation:\n\nPerformance Results:\n\nTest clip: 1280×720 @ 59.94 FPS, 1,400 frames\nExtraction @ 30 FPS: 699 frames in 2.58s (~271 FPS processing)\nStorage reduction: ~50% vs 60 FPS\nMaintains motion detail for tracking\n\nImpact:\n\nFaster training & iteration\nLower storage footprint\nConsistent timestamps for alignment\nFoundation for all downstream tracking\n\n\n\nKeaton Ruthardt: Player Tracking System\nObjective: Detect and track outfielders using YOLOv8\nSystem Architecture:\nclass SimpleOutfielderTracker:\n    \"\"\"\n    Simplified tracker that only detects real outfielders.\n    Stricter filtering to avoid false positives.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_size: str = 'n',\n        confidence_threshold: float = 0.25,\n        device: str = 'cpu'\n    ):\n        self.confidence_threshold = confidence_threshold\n        self.device = device\n\n        model_name = f'yolov8{model_size}.pt'\n        logger.info(f\"Loading {model_name}\")\n        self.model = YOLO(model_name)\n\n        self.tracker = SimpleCentroidTracker(max_disappeared=30, max_distance=150)\n        self.prev_frame = None\n\n    def detect_players(self, frame: np.ndarray) -&gt; List[Dict]:\n        \"\"\"Detect all people in frame.\"\"\"\n        results = self.model.predict(\n            frame,\n            conf=self.confidence_threshold,\n            iou=0.3,\n            classes=[0],\n            verbose=False,\n            device=self.device,\n            imgsz=1280\n        )\n\n        detections = []\n        if len(results) &gt; 0 and results[0].boxes is not None:\n            boxes = results[0].boxes\n            for i in range(len(boxes)):\n                xyxy = boxes.xyxy[i].cpu().numpy()\n                conf = float(boxes.conf[i].cpu().numpy())\n                x1, y1, x2, y2 = map(int, xyxy)\n                cx = int((x1 + x2) / 2)\n                cy = int((y1 + y2) / 2)\n                area = (x2 - x1) * (y2 - y1)\n\n                detections.append({\n                    'bbox': [x1, y1, x2, y2],\n                    'center': [cx, cy],\n                    'confidence': conf,\n                    'area': area\n                })\n\n        return detections\n\n    def filter_to_real_outfielders(\n        self,\n        detections: List[Dict],\n        frame_shape: tuple\n    ) -&gt; List[Dict]:\n        \"\"\"\n        STRICT filtering to only get real outfielders in the field.\n\n        Rules:\n        - Must be in MIDDLE vertical region (not top where fans/scoreboards are)\n        - Must be reasonable size (not huge closeups, not tiny distant objects)\n        - Must be in field area (centered horizontally)\n        \"\"\"\n        height, width = frame_shape\n\n        outfielders = []\n        for det in detections:\n            cx, cy = det['center']\n            area = det['area']\n\n            # STRICT RULES to avoid false positives:\n\n            # 1. Must be in MIDDLE of frame vertically (30% - 75%)\n            #    This excludes fans/scoreboards at top (y &lt; 30%)\n            if not (height * 0.30 &lt; cy &lt; height * 0.75):\n                continue\n\n            # 2. Size constraints\n            if area &lt; 3000 or area &gt; 35000:\n                continue\n\n            # 3. Must be reasonably centered horizontally (10% - 90%)\n            if not (width * 0.10 &lt; cx &lt; width * 0.90):\n                continue\n\n            outfielders.append(det)\n\n        return outfielders\n\n    def detect_ball(self, frame: np.ndarray) -&gt; Optional[List[int]]:\n        \"\"\"Detect ball position.\"\"\"\n        results = self.model.predict(\n            frame,\n            conf=0.15,\n            iou=0.5,\n            classes=[32],  # Sports ball\n            verbose=False,\n            device=self.device,\n            imgsz=1280\n        )\nKey Features:\n\nAdaptive Sizing:\n\nDynamic min-area based on candidate count\n7,200 px² when &gt;6 candidates (filter fans)\n2,500 px² in sparse frames (capture distant players)\n\nOutfielder Focus: Vertical band filter (Y ≈ 0.10–0.75)\nPosition Assignment: Sort by x-coordinate → LF (left), CF (center), RF (right)\nWindowing: Process only play frames (150–240) to save compute\n\nOutputs:\n\nAnnotated MP4 with LF/CF/RF labels and spacing lines\nCSV per frame with player positions and distances\nBall and catch detection flags\n\nResults:\n\nVisibility rate: 10.8% of frames had classifiable outfielders\nCatch identification: 2 catches detected (frames 205 & 216)\nSpacing metrics: LF-CF, CF-RF, LF-RF Euclidean distances\n\n\n\nDuoduo Cai: Spatial Mapping System\nObjective: Convert 2D pixels to real-world field positions",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2: Vision Pipeline and Feature Engineering</span>"
    ]
  },
  {
    "objectID": "week2.html#introduction",
    "href": "week2.html#introduction",
    "title": "3  Week 2: Vision Pipeline and Feature Engineering",
    "section": "3.3 Introduction",
    "text": "3.3 Introduction\nThis prototype demonstrates converting 2D pixel coordinates from a baseball video frame\ninto real-world field positions (feet) using homography transformation.\n\nStep 1: Import libraries and load image\nThe baseball field picture (field_frame.png) has been loaded and displayed. Users are instructed to click four critical spots on the field—Home, 1B, 2B, and 3B—to specify the homography transition.\nimport matplotlib\nmatplotlib.use('TkAgg')\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nimg = Image.open(\"field_frame.png\")\nplt.imshow(img)\nplt.title(\"Click four points: Home → 1B → 2B → 3B\")\nplt.show()\n\n\nStep 2: Interactively click on the four dots and print the result\nAfter clicking on the four bases, the pixel coordinates are printed. For example: [(1021.12, 1131.32), (2037.17, 753.56), (1062.81, 141.33), (7.69, 745.74)]. These are the positions of the bases in the image (pixels).\n\npoints = plt.ginput(4, timeout = 0)\nplt.close()\nprint(\"Clicked pixel coordinates:\")\nprint(points)\n\n\n\nStep 3: Auto-order to [Home, 1B, 2B, 3B]\nThe clicked points are automatically ordered as Home → 1B → 2B → 3B. This ensures consistency when computing the transformation matrix.\n\npoints_raw = np.array(points, dtype=float)\nys = points_raw[:,1]; xs = points_raw[:,0]\nhome_idx   = np.argmax(ys)\nsecond_idx = np.argmin(ys)\nremain     = [i for i in range(4) if i not in (home_idx, second_idx)]\nfirst_idx  = remain[np.argmax(xs[remain])]   # right-bottom\nthird_idx  = remain[np.argmin(xs[remain])]   # left-bottom\nsrc_pts = np.array([\n    points_raw[home_idx],   # Home\n    points_raw[first_idx],  # 1B\n    points_raw[second_idx], # 2B\n    points_raw[third_idx],  # 3B\n], dtype=float)\nprint(\"ordered src_pts:\\n\", src_pts)\n\n\n\nStep 4: Homography (DLT) and mapping utils\nThe Direct Linear Transform (DLT) algorithm generates the homography matrix (H), which maps picture pixels to real field coordinates (feet).\n\ndst_pts = np.array([[0,0],[90,0],[90,90],[0,90]], dtype=float)\n\ndef compute_homography(src, dst):\n    A=[]\n    for (x,y),(X,Y) in zip(src,dst):\n        A += [[-x,-y,-1, 0, 0, 0, x*X, y*X, X],\n              [ 0, 0, 0,-x,-y,-1, x*Y, y*Y, Y]]\n    U,S,Vt = np.linalg.svd(np.asarray(A))\n    h = Vt[-1] / Vt[-1,-1]\n    return h.reshape(3,3)\n\ndef apply_homography(H, pts):\n    pts = np.asarray(pts, float)\n    hom = np.hstack([pts, np.ones((pts.shape[0],1))])\n    trans = (H @ hom.T).T\n    trans[:,:2] /= trans[:,[2]]\n    return trans[:,:2]\n\nH = compute_homography(src_pts, dst_pts)\nprint(\"H:\\n\", H)\n\n# Save the H calculated last week to the desktop deliverable3 project\nimport numpy as np, os\nBASE = os.path.expanduser(\"~/Desktop/deliverable3_feature_extraction\")\nout_path = os.path.join(BASE, \"data\", \"homography\", \"H.npy\")\nos.makedirs(os.path.dirname(out_path), exist_ok=True)\n\nnp.save(out_path, H.astype(np.float32))\n\n# Verify\nH_check = np.load(out_path)\nprint(\"Saved to:\", out_path, \"shape:\", H_check.shape, \"dtype:\", H_check.dtype)\n\n\n\nStep 5: Sanity Check (Homography Validation).\nTo evaluate the correctness of the homography transformation, we reprojected four manually chosen base points (Home, 1B, 2B, and 3B) into real-world field coordinates.\nThe coordinates found were about Home (0.0, 0.0), 1B (90.0, 0.0), 2B (90.0, 90.0), and 3B (0.0, 90.0), completely matching the normal baseball infield dimensions of 90 × 90 ft. This indicates that the transformation matrix accurately preserves spatial geometry, making the pixel-to-field mapping reliable for player position estimation.\n\ncheck_ft = apply_homography(H, src_pts)\nprint(\"mapped 4 corners (feet):\\n\", check_ft)\n\n\nStep 6: Convert any pixel (click one or fill manually)\nA new pixel point (such as the runner’s foot) is picked. It is translated from pixel coordinates (842.32, 395.44) to field coordinates (65.93 ft, 81.76 ft), demonstrating successful 2D-to-real-world mapping.\n\nplt.figure(figsize=(6,4), dpi=110)\nplt.imshow(img); plt.title(\"Click runner foot (1 point)\")\nrunner_click = plt.ginput(1, timeout=0)\nplt.close()\nrunner_px = np.array(runner_click, dtype=float)\n\n# runner_px = np.array([[1200, 900]], dtype=float)  # Manual values can be used as substitutes\n\nrunner_ft = apply_homography(H, runner_px)\nprint(\"runner pixel -&gt; feet:\", runner_px, \"=&gt;\", runner_ft)\n\n\n\nStep 7: Overlay labels for report\nThe selected base points (Home, 1B, 2B, and 3B) are superimposed on the image with yellow labels to ensure that all points were accurately registered throughout the transformation process.\n\nplt.figure(figsize=(6,4), dpi=120)\nplt.imshow(img)\nlabels = [\"Home\",\"1B\",\"2B\",\"3B\"]\nfor (x,y),lab in zip(src_pts, labels):\n    plt.scatter([x],[y], s=110, facecolors='none', edgecolors='yellow', linewidths=2)\n    plt.text(x+8, y-8, lab, color='yellow',\n             bbox=dict(facecolor='black', alpha=0.55, pad=2), fontsize=9)\nplt.title(\"Selected points (Home → 1B → 2B → 3B)\")\nplt.axis('off')\nplt.show()\n# plt.savefig(\"selected_points_overlay.png\", dpi=160, bbox_inches='tight')\n\n\n\n\nStep 8: Summary of Conversion\nThe validated homography matrix was used to map the selected player’s foot to real-world coordinates of about (X = 65.93 ft, Y = 81.76 ft). This point is fairly close to the pitcher’s mound (approximately 60.5 feet from home plate), demonstrating the geometric correctness and reliability of our coordinate translation. The results show that our homography-based method can successfully transform 2D pixel positions from a video frame to genuine on-field locations with realistic precision.\nprint(check_ft)\n\npt = np.round(runner_ft, 2) \nprint(f\"Runner (feet) ≈ X={pt[0,0]} ft, Y={pt[0,1]} ft\")\n\n\n\nStep 9: Measure distance between two outfielders\n\n# 9.1 Load the same broadcast frame and ask the user to click two outfielders\nimg = Image.open(\"outfield_frame.png\")\nplt.figure(figsize=(7,4), dpi=120)\nplt.imshow(img)\nplt.title(\"Click two outfielders (left → right)\")\nplt.axis('off')\nplt.show()\n\n# Collect two pixel coordinates (click left OF first, then right OF)\nof_px = plt.ginput(2, timeout=0)     # returns [(x1,y1), (x2,y2)]\nplt.close()\nprint(\"Clicked pixel coords (outfielders):\", of_px)\n\n# 9.2 Convert pixel coordinates to field coordinates (feet) via homography\nof_ft = apply_homography(H, np.array(of_px, dtype=float))\nprint(\"Outfielders in field coordinates (feet):\\n\", np.round(of_ft, 2))\n\n# 9.3 Compute Euclidean distance between the two outfielders in feet\ndist_ft = float(np.linalg.norm(of_ft[0] - of_ft[1]))\nprint(f\"Distance between outfielders ≈ {dist_ft:.2f} ft\")\n\n# 9.4 (Optional) Visual overlay for the report\nplt.figure(figsize=(7,4), dpi=140)\nplt.imshow(img)\nplt.axis('off')\nplt.title(f\"Outfielders distance ≈ {dist_ft:.1f} ft\")\nfor (x, y), label in zip(of_px, [\"OF-1\", \"OF-2\"]):\n    plt.scatter([x], [y], s=120, c='cyan', marker='x', linewidths=2)\n    plt.text(x+10, y-10, label, color='cyan',\n             bbox=dict(facecolor='black', alpha=0.55, pad=2), fontsize=9)\nplt.tight_layout()\n# plt.savefig(\"outfielders_distance_overlay.png\", dpi=160, bbox_inches='tight')\nplt.show()\nThe results demonstrate that two outfielders were manually selected from the video frame, with pixel coordinates (367.85, 1108.08) and (1863.53, 1174.72). These pixel positions were then converted into real-world field coordinates using the previously computed homography matrix, yielding approximations of (-31.39, 38.27) ft and (41.13, -54.64) ft.\nThe software calculated the Euclidean distance between these two positions and decided that the outfielders are separated by around 117.9 feet on the field. This figure is appropriate for a Major League Baseball outfield arrangement in which players normally maintain a spacing of 110-130 feet, depending on stadium geometry and defensive placement.\nThe visualization additionally marks the two selected positions as “OF-1” and “OF-2” on the image, indicating that the transformation correctly transfers on-screen player placements to their real-world distances.\n\n\nAI Assistance Statement\nAI assistance: ChatGPT was used extensively in this deliverable. It helps me understand better what I should do for this week’s deliverable, and it guides me step by step with my prompt. The majority of the code structure, Python functions, and written explanations were initially generated with AI assistance. I used ChatGPT to help me draft prototype code for homography computation, coordinate transformation, visualization, and distance calculation. I then executed all code myself, debugged errors, verified the homography outputs, selected the calibration points, confirmed the pixel-to-field mapping, and interpreted every result.\nThis document reflects my understanding of the method, while the AI tool assisted me in writing and structuring the initial code and explanations.\nHomography transformation to map pixel coordinates to field coordinates:\n\nQuality Checks:\n\nBase square ≈ 90 ft sides (±2 ft)\nFoul lines approximately orthogonal\nResidual reprojection error ≤ 2 px\nReject if fewer than 4 reliable points\n\nImpact:\n\nConverts video into model-ready geometry (feet)\nMakes features comparable across games/stadiums/cameras\nEnables true distance calculations for outfield_spread_ft\n\n\n\nSamuel Bulnes: Model Accuracy Testing\nObjective: Optimize feature set while maintaining performance\nFeature Reduction Analysis:\nOriginal model: 45 features → Optimized: 10 features (-78% complexity)\nTop 10 Features by Importance:\n\nrunner_base (28.6%)\nhit_distance (27.9%)\nbearing (10.5%)\nlaunch_angle (7.5%)\nlaunch_direction (6.9%)\nexit_speed (4.8%)\nhangtime (4.5%)\nexit_speed_squared (4.4%)\nfielder_pos (2.5%)\noutfield_spread_event (2.5%)\n\nPerformance Validation:\n\n\n\nModel\n45 Features\n10 Features\nChange\n\n\n\n\nRandom Forest\n0.3951\n0.3795\n+3.9%\n\n\nGradient Boosting\n0.3647\n0.3890\n-6.7%\n\n\nLogistic Regression\n0.4200\n0.4416\n-5.1%\n\n\n\nBest Model: Random Forest with 10 features (Log Loss = 0.3795)\nImpact:\n\n70%+ reduction in data prep and training time\nMaintains core predictive signal\nFaster retraining and lighter deployment\nReady for production scaling",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2: Vision Pipeline and Feature Engineering</span>"
    ]
  },
  {
    "objectID": "week2.html#technical-achievements",
    "href": "week2.html#technical-achievements",
    "title": "3  Week 2: Vision Pipeline and Feature Engineering",
    "section": "3.4 Technical Achievements",
    "text": "3.4 Technical Achievements\n\n1. Computer Vision Pipeline\n\nYOLOv8 Integration: Real-time player detection\nPosition Classification: Automatic LF/CF/RF assignment\nCatch Detection: Ball-to-player distance calculation\nID Persistence: Track players across frames\n\n\n\n2. Spatial Features\n\nOutfield Spread: Quantitative field coverage metric\nFielder Position: Categorical catch assignment\nPixel-to-Feet Conversion: Real-world distance calculations\nMulti-Camera Support: Homography-based calibration\n\n\n\n3. Model Optimization\n\nFeature Selection: 45 → 10 features with minimal loss\nPerformance Validation: Cross-validation across models\nImportance Analysis: Identify key predictive features",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2: Vision Pipeline and Feature Engineering</span>"
    ]
  },
  {
    "objectID": "week2.html#challenges-solutions",
    "href": "week2.html#challenges-solutions",
    "title": "3  Week 2: Vision Pipeline and Feature Engineering",
    "section": "3.5 Challenges & Solutions",
    "text": "3.5 Challenges & Solutions\n\nChallenge 1: Player Detection in Crowds\nProblem: YOLO detects fans and non-players\nSolution: Adaptive area thresholds + vertical band filtering\n\n\nChallenge 2: Camera Angle Variations\nProblem: Different stadiums/broadcasts have different perspectives\nSolution: Homography calibration per video with quality checks\n\n\nChallenge 3: Model Complexity\nProblem: 45 features slow down training and deployment\nSolution: Feature importance analysis → keep top 10 features",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2: Vision Pipeline and Feature Engineering</span>"
    ]
  },
  {
    "objectID": "week2.html#key-metrics",
    "href": "week2.html#key-metrics",
    "title": "3  Week 2: Vision Pipeline and Feature Engineering",
    "section": "3.6 Key Metrics",
    "text": "3.6 Key Metrics\n\nFrame Processing: 271 FPS extraction speed\nPlayer Detection: 10.8% frame visibility rate\nModel Performance: Log Loss 0.3795 (10 features)\nComplexity Reduction: 78% fewer features\nStorage Savings: 50% reduction with 30 FPS\n\n\nWeek 2 transformed raw video into structured, model-ready data through advanced computer vision and feature engineering.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2: Vision Pipeline and Feature Engineering</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI Baseball Runner Advance Prediction",
    "section": "",
    "text": "1 AI Baseball Runner Advance Prediction",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>AI Baseball Runner Advance Prediction</span>"
    ]
  },
  {
    "objectID": "index.html#project-overview",
    "href": "index.html#project-overview",
    "title": "AI Baseball Runner Advance Prediction",
    "section": "1.1 Project Overview",
    "text": "1.1 Project Overview\nThis book documents the development of an end-to-end AI system that predicts whether a baserunner will successfully advance on sacrifice fly plays in baseball. Combining computer vision and machine learning, our system processes MLB video footage to deliver real-time SAFE/OUT predictions with confidence scores.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>AI Baseball Runner Advance Prediction</span>"
    ]
  },
  {
    "objectID": "index.html#the-challenge",
    "href": "index.html#the-challenge",
    "title": "AI Baseball Runner Advance Prediction",
    "section": "1.2 The Challenge",
    "text": "1.2 The Challenge\nWhen a sacrifice fly occurs in baseball, runners must decide whether to attempt advancing to the next base. This split-second decision depends on multiple factors:\n\nBall trajectory and hang time\nOutfielder positioning and distance\nRunner starting base\n\nOur system automates this prediction using: - YOLOv8 for player detection - Custom tracking algorithms for position monitoring - Spatial feature engineering for geometry calculations - Ensemble machine learning for outcome prediction",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>AI Baseball Runner Advance Prediction</span>"
    ]
  },
  {
    "objectID": "index.html#system-pipeline",
    "href": "index.html#system-pipeline",
    "title": "AI Baseball Runner Advance Prediction",
    "section": "1.3 System Pipeline",
    "text": "1.3 System Pipeline\nMLB Video Input\n      ↓\nYOLOv8 Player Detection\n      ↓\nPosition Tracking\n      ↓\nFeature Extraction\n      ↓\nML Ensemble Model\n      ↓\nSAFE/OUT Prediction\n      ↓\nAnnotated Video Output",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>AI Baseball Runner Advance Prediction</span>"
    ]
  },
  {
    "objectID": "index.html#key-features",
    "href": "index.html#key-features",
    "title": "AI Baseball Runner Advance Prediction",
    "section": "1.4 Key Features",
    "text": "1.4 Key Features\n\nComputer Vision\n\nReal-time player detection using YOLOv8\nMulti-player tracking with persistent IDs\nAutomatic catch detection\nSpatial coordinate mapping (pixels → feet)\n\n\n\nMachine Learning\n\nEnsemble model (Random Forest + Gradient Boosting + Logistic Regression)\n7 optimized features\n89% AUC performance\nLow log loss (0.40)\n\n\n\nPipeline\n\nSingle-command execution\n~53 seconds per video\nComprehensive outputs (CSV, video, predictions)\nProduction-ready deployment",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>AI Baseball Runner Advance Prediction</span>"
    ]
  },
  {
    "objectID": "index.html#project-timeline",
    "href": "index.html#project-timeline",
    "title": "AI Baseball Runner Advance Prediction",
    "section": "1.5 Project Timeline",
    "text": "1.5 Project Timeline\n\n\n\n\n\n\n\n\nWeek\nFocus\nKey Deliverables\n\n\n\n\nWeek 1\nSetup & Planning\nEnvironment, data sources, team roles\n\n\nWeek 2\nVision Pipeline\nFrame extraction, YOLO detection, feature engineering\n\n\nWeek 3\nData Collection\n~50 videos, validation scripts, model integration\n\n\nWeek 4\nComplete Pipeline\nEnd-to-end automation, performance testing\n\n\nWeek 5\nFinal Testing\n~120 video dataset, documentation, demo",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>AI Baseball Runner Advance Prediction</span>"
    ]
  },
  {
    "objectID": "index.html#team",
    "href": "index.html#team",
    "title": "AI Baseball Runner Advance Prediction",
    "section": "1.6 Team",
    "text": "1.6 Team\n\nTeam Leaders: Joshua Cano, Keaton Ruthardt\nTeam Members: Diego Mendoza, Samuel Bulnes, Duoduo Cai",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>AI Baseball Runner Advance Prediction</span>"
    ]
  },
  {
    "objectID": "index.html#repository-structure",
    "href": "index.html#repository-structure",
    "title": "AI Baseball Runner Advance Prediction",
    "section": "1.7 Repository Structure",
    "text": "1.7 Repository Structure\nAIBaseballProject/\n├── main/                      # Core pipeline code\n│   ├── run_complete_pipeline.py\n│   ├── simple_tracker.py\n│   ├── pixel_to_feet_converter.py\n│   ├── feature_preparation.py\n│   └── original_ensemble_model_D3.py\n│\n├── book/                      # This documentation\n│   ├── index.qmd\n│   ├── week1.qmd\n│   ├── week2.qmd\n│   ├── week3.qmd\n│   ├── week4.qmd\n│   └── week5.qmd\n│\n├── deliverables/              # Weekly progress\n├── results/                   # Output predictions\n└── README.md                  # Quick start guide",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>AI Baseball Runner Advance Prediction</span>"
    ]
  },
  {
    "objectID": "index.html#quick-start",
    "href": "index.html#quick-start",
    "title": "AI Baseball Runner Advance Prediction",
    "section": "1.8 Quick Start",
    "text": "1.8 Quick Start\n\nInstallation\n# Clone repository\ngit clone https://github.com/Keaton-Ruthardt/AIBaseballProject.git\ncd AIBaseballProject\n\n# Create virtual environment\npython3 -m venv venv\nsource venv/bin/activate\n\n# Install dependencies\npip install -r requirements.txt\n\n\nRun Pipeline\ncd main\npython3 run_complete_pipeline.py --video videos/sac_fly_001.mp4 --metadata video_metadata.csv --output results\n\n\nOutput\nresults/\n├── sac_fly_001_tracker.csv      # Player positions per frame\n├── sac_fly_001_features.csv     # Model-ready features\n├── sac_fly_001_prediction.txt   # SAFE/OUT + probability\n└── sac_fly_001_annotated.mp4    # Video with overlays",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>AI Baseball Runner Advance Prediction</span>"
    ]
  },
  {
    "objectID": "index.html#performance-metrics",
    "href": "index.html#performance-metrics",
    "title": "AI Baseball Runner Advance Prediction",
    "section": "1.9 Performance Metrics",
    "text": "1.9 Performance Metrics\n\n\n\nMetric\nValue\n\n\n\n\nAUC\n0.89\n\n\nLog Loss\n0.40\n\n\nProcessing Time\n~53s/video\n\n\nSuccess Rate\n60-70%\n\n\nFeature Count\n8",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>AI Baseball Runner Advance Prediction</span>"
    ]
  },
  {
    "objectID": "index.html#key-technologies",
    "href": "index.html#key-technologies",
    "title": "AI Baseball Runner Advance Prediction",
    "section": "1.10 Key Technologies",
    "text": "1.10 Key Technologies\n\nPython 3.10+: Core language\nYOLOv8: Object detection\nOpenCV: Video processing\nscikit-learn: Machine learning\npandas: Data manipulation\nNumPy: Numerical operations",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>AI Baseball Runner Advance Prediction</span>"
    ]
  },
  {
    "objectID": "index.html#documentation-structure",
    "href": "index.html#documentation-structure",
    "title": "AI Baseball Runner Advance Prediction",
    "section": "1.11 Documentation Structure",
    "text": "1.11 Documentation Structure\nThis book is organized chronologically, documenting our 5-week development journey:\n\nWeek 1: Setup - Project initialization and planning\nWeek 2: Vision Pipeline - Core computer vision development\nWeek 3: Data Collection - Dataset creation and validation\nWeek 4: Complete Pipeline - Integration and optimization\nWeek 5: Final Testing - Comprehensive evaluation and delivery\n\nEach chapter details:\n\nTeam contributions\nTechnical implementations\nChallenges and solutions\nCode examples\nPerformance metrics\nKey learnings",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>AI Baseball Runner Advance Prediction</span>"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "AI Baseball Runner Advance Prediction",
    "section": "1.12 Acknowledgments",
    "text": "1.12 Acknowledgments\nSpecial thanks to:\n\nDr. V for project guidance\nMLB for public video and Statcast data\nUltralytics for YOLOv8 framework\nOpen source community for tools and libraries\n\n\nThis project demonstrates the power of AI in sports analytics, combining cutting-edge computer vision with machine learning to solve real-world baseball prediction challenges.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>AI Baseball Runner Advance Prediction</span>"
    ]
  },
  {
    "objectID": "index.html#navigate-this-book",
    "href": "index.html#navigate-this-book",
    "title": "AI Baseball Runner Advance Prediction",
    "section": "1.13 Navigate This Book",
    "text": "1.13 Navigate This Book\nUse the sidebar to explore each week’s progress, or start with Week 1 to follow our complete development journey.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>AI Baseball Runner Advance Prediction</span>"
    ]
  },
  {
    "objectID": "week3.html",
    "href": "week3.html",
    "title": "4  Week 3: Data Collection and Pipeline Integration",
    "section": "",
    "text": "4.1 Overview\nWeek 3 focused on building a robust validation dataset and integrating all pipeline components into a cohesive system. The team collected real MLB sacrifice fly videos, validated tracking outputs, and connected computer vision to machine learning.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3: Data Collection and Pipeline Integration</span>"
    ]
  },
  {
    "objectID": "week3.html#project-pipeline",
    "href": "week3.html#project-pipeline",
    "title": "4  Week 3: Data Collection and Pipeline Integration",
    "section": "4.2 Project Pipeline",
    "text": "4.2 Project Pipeline\nThe complete workflow established this week:\nDiego & Duoduo: Manual Data Collection \n    ↓\nKeaton: Player Tracking System (simple_tracker.py)\n    ↓\nJoshua: Player Detection + Validation (validator.py)\n    ↓\nSamuel: Model Integration & Testing",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3: Data Collection and Pipeline Integration</span>"
    ]
  },
  {
    "objectID": "week3.html#team-contributions",
    "href": "week3.html#team-contributions",
    "title": "4  Week 3: Data Collection and Pipeline Integration",
    "section": "4.3 Team Contributions",
    "text": "4.3 Team Contributions\n\nDiego Mendoza & Duoduo Cai: Manual Video Collection\nObjective: Build validation dataset through systematic video collection\nTool Development: manual_download_helper.py\n\n\nResults:\n\n~25 videos collected from MLB Film Room\nEach video manually annotated with:\n\nOutcome (SAFE/OUT)\nExit velocity\nLaunch angle\nHit distance\nHang time\nRunner starting base\n\nValidation CSV created for downstream testing\n\n\nImpact:\n\nGround truth dataset for model validation\nDiverse play scenarios captured\nQuality-controlled annotations\nFoundation for accuracy testing\n\n\n\nKeaton Ruthardt: Enhanced Player Tracking System\nObjective: Robust outfielder tracking with catch detection\nSystem Flow:\nVideo Input\n    ↓\nDetect Players & Ball (YOLOv8)\n    ↓\nTrack Positions (Unique IDs)\n    ↓\nIdentify Catcher (Distance-based)\n    ↓\nExport CSV File\nKey Implementation:\nThe following code processes each frame and video, and exports the collected data generated from the functions into a CSV.\n def process_frame(self, frame: np.ndarray, frame_number: int) -&gt; Dict:\n        \"\"\"Process single frame.\"\"\"\n        height, width = frame.shape[:2]\n\n        # Detect all people\n        all_detections = self.detect_players(frame)\n\n        # Filter to real outfielders only\n        outfielder_detections = self.filter_to_real_outfielders(\n            all_detections, (height, width)\n        )\n\n        # Track outfielders\n        tracked_players = self.tracker.update(outfielder_detections)\n\n        # Detect ball\n        ball_position = self.detect_ball(frame)\n\n        # Determine catcher\n        catcher_id = None\n        catcher_distance = None\n\n        if ball_position and len(tracked_players) &gt; 0:\n            min_dist = float('inf')\n            closest_id = None\n\n            for player_id, player_data in tracked_players.items():\n                px, py = player_data['center']\n                bx, by = ball_position\n\n                distance = np.sqrt((px - bx)**2 + (py - by)**2)\n                if distance &lt; min_dist:\n                    min_dist = distance\n                    closest_id = player_id\n\n            # Only count as catch if within 120 pixels\n            if min_dist &lt; 120:\n                catcher_id = closest_id\n                catcher_distance = min_dist\n\n        self.prev_frame = frame.copy()\n\n        return {\n            'frame_number': frame_number,\n            'tracked_players': tracked_players,\n            'ball_position': ball_position,\n            'catcher_id': catcher_id,\n            'catcher_distance': catcher_distance\n        }\n\n    def process_video(\n        self,\n        video_path: str,\n        output_path: Optional[str] = None,\n        start_frame: int = 0,\n        end_frame: Optional[int] = None\n    ) -&gt; List[Dict]:\n        \"\"\"Process video and return results.\"\"\"\n        cap = cv2.VideoCapture(video_path)\n        if not cap.isOpened():\n            raise ValueError(f\"Cannot open video: {video_path}\")\n\n        fps = int(cap.get(cv2.CAP_PROP_FPS))\n        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n        logger.info(f\"Video: {video_path}\")\n        logger.info(f\"Resolution: {width}x{height}, FPS: {fps}, Total frames: {total_frames}\")\n\n        writer = None\n        if output_path:\n            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n            writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\n        results = []\n        frame_idx = 0\n\n        if start_frame &gt; 0:\n            cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n            frame_idx = start_frame\n\n        try:\n            while True:\n                ret, frame = cap.read()\n                if not ret:\n                    break\n                if end_frame and frame_idx &gt;= end_frame:\n                    break\n\n                result = self.process_frame(frame, frame_idx)\n                results.append(result)\n\n                # Visualize\n                if writer:\n                    vis = frame.copy()\n\n                    # Draw tracked players\n                    for player_id, player_data in result['tracked_players'].items():\n                        x1, y1, x2, y2 = player_data['bbox']\n                        cx, cy = player_data['center']\n\n                        color = (0, 255, 0)  # Green\n                        if result['catcher_id'] == player_id:\n                            color = (0, 255, 255)  # Yellow for catcher\n\n                        cv2.rectangle(vis, (x1, y1), (x2, y2), color, 2)\n                        cv2.putText(vis, f\"Player {player_id}\", (x1, y1-10),\n                                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n                        cv2.circle(vis, (cx, cy), 5, color, -1)\n\n                    # Draw ball\n                    if result['ball_position']:\n                        bx, by = result['ball_position']\n                        cv2.circle(vis, (bx, by), 10, (0, 255, 255), 3)\n\n                    # Draw catch event\n                    if result['catcher_id'] is not None:\n                        cv2.putText(vis, f\"CATCH by Player {result['catcher_id']}\",\n                                   (10, vis.shape[0] - 20),\n                                   cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 255), 3)\n\n                    writer.write(vis)\n\n                if frame_idx % 30 == 0:\n                    logger.info(f\"Frame {frame_idx}: {len(result['tracked_players'])} players tracked\")\n\n                frame_idx += 1\n\n        finally:\n            cap.release()\n            if writer:\n                writer.release()\n                logger.info(f\"Saved: {output_path}\")\n\n        logger.info(f\"Processed {len(results)} frames\")\n        return results\n\n    def export_simple_csv(self, results: List[Dict], output_csv: str):\n        \"\"\"\n        Export SIMPLE CSV with just:\n        - Frame number\n        - Outfielder positions (x, y for each player)\n        - Who caught the ball\n        \"\"\"\n        import pandas as pd\n\n        rows = []\n        for r in results:\n            row = {\n                'frame_number': r['frame_number'],\n                'num_outfielders_detected': len(r['tracked_players']),\n            }\n\n            # Add each tracked player's position\n            player_ids = sorted(r['tracked_players'].keys())\n            for player_id in player_ids:\n                player_data = r['tracked_players'][player_id]\n                row[f'player_{player_id}_x'] = player_data['center'][0]\n                row[f'player_{player_id}_y'] = player_data['center'][1]\n\n            # Catch info - who caught the ball\n            row['catcher_player_id'] = r['catcher_id'] if r['catcher_id'] is not None else -1\n\n            rows.append(row)\n\n        df = pd.DataFrame(rows)\n\n        # Fill NaN with empty for player positions that don't exist\n        df = df.fillna('')\n\n        df.to_csv(output_csv, index=False)\n        logger.info(f\"Exported to: {output_csv}\")\n\n        # Summary\n        players_tracked = set()\n        for r in results:\n            players_tracked.update(r['tracked_players'].keys())\n\n        logger.info(f\"Total unique players tracked: {len(players_tracked)}\")\n        logger.info(f\"Player IDs: {sorted(players_tracked)}\")\n\n        return df\n\n\nif __name__ == \"__main__\":\n    print(\"Simple Outfielder Tracker loaded\")\nCSV Output Structure:\n\n\n\nColumn\nDescription\n\n\n\n\nframe_number\nFrame index in video\n\n\nnum_outfielders_detected\nCount of detected players (0-3)\n\n\nplayer_0_x, player_0_y\nPixel coordinates of player 0\n\n\nplayer_1_x, player_1_y\nPixel coordinates of player 1\n\n\nplayer_2_x, player_2_y\nPixel coordinates of player 2\n\n\ncatcher_player_id\nID of player who caught ball (-1 if none)\n\n\n\n\nFeatures:\n\nUnique Player IDs: Consistent tracking across frames\nCatch Detection: Distance threshold (120 px) for ball-player proximity\nMulti-Player Support: Handles 0-3 outfielders per frame\nCoordinate Export: Pixel positions for all detected players\n\n\n\nJoshua Cano: Validation Script\nObjective: Verify tracking CSV data quality before model input\nOverview\nThis week’s task focused on verifying that our video-processing pipeline correctly outputs the essential features our model needs:\n\nFielder positions (x, y pixel coordinates)\nDetection of a catch event\nMeasurement of distance between multiple outfielders\n\nTo do this, we created a Python script called validator.py. This script automatically analyzes any CSV file produced by Keaton’s tracker (for example, simple_output.csv or Two Outfielder.csv) and reports whether these three key elements are present and functioning.\n1. Player detection:\nIt looks for all columns named in the format player_{id}x and player{id}_y. These indicate that one or more players were successfully tracked in each video frame.\n\n2. Catch detection:\nIt checks the column catcher_player_id. If the value is 0 or greater in any frame, it means a catch was detected and identifies which player caught the ball\n\n3. Distance measurement:\nFor frames containing two or more players, it calculates the distance between every pair of players using the Euclidean distance formula:\n\nThe script then produces two output files:\n\nvalidation_report.csv – A concise summary for each video file analyzed.\n\n\nColumn Explanation\n\n\nvalidation_report_pairwise.csv – An optional detailed file listing per-frame distances between players (used for debugging and visualization).\n\nA snippet from the detailed per-frame distance file shows:\n\nplayer_a = 0 and player_b = 1 identify the two tracked fielders.\nThe ax, ay, bx, by columns are their pixel coordinates in that frame.\ndistance_px is calculated using the Euclidean formula, giving the exact pixel separation between them.\n\n\nQuality Assurance:\n\nEnsures player coordinate columns exist\nVerifies catch detection data present\nCalculates inter-player distances\nFlags missing or invalid data\nPrevents bad data from reaching model\n\nClaude AI was used to help build this validator script, specifically for helping process Keaton’s CSV inputs.\n\n\nSamuel Bulnes: Enhanced Model Integration\nObjective: Integrate full tracking CSV with ML model\nModel Pipeline:\n\n\nKey Functions:\n\nload_data(): Automatically loads updated CSVs\npreprocess_data(): Merges tracking features with runner features\nFully compatible with Joshua’s validated datasets\n\nEngineering Features:\nFrom tracking CSV, the model now computes: - Player spacing (distance between outfielders) - Movement dynamics (velocity, direction) - Positional alignment at catch time - Field coverage metrics\nResults:\n\n✅ Training data: Loaded successfully\n✅ Test data: Loaded successfully\n\n✅ Position tracking: Loaded successfully\n✅ Feature engineering: 9 features generated\n✅ Pipeline: Complete data-to-model flow achieved",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3: Data Collection and Pipeline Integration</span>"
    ]
  },
  {
    "objectID": "week3.html#technical-achievements",
    "href": "week3.html#technical-achievements",
    "title": "4  Week 3: Data Collection and Pipeline Integration",
    "section": "4.4 Technical Achievements",
    "text": "4.4 Technical Achievements\n\n1. Validation Dataset\n\n25 manually annotated videos\nGround truth labels (SAFE/OUT)\nStatcast metadata captured\nQuality-controlled collection process\n\n\n\n2. Tracking System\n\nYOLOv8-based detection\nMulti-player tracking with IDs\nAutomatic catch detection\nStructured CSV exports\n\n\n\n3. Data Validation\n\nAutomated quality checks\nDistance calculations\nMissing data detection\nPre-model validation\n\n\n\n4. Model Integration\n\nFull tracking CSV ingestion\nFeature engineering automation\nSeamless data flow\n9 engineered features",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3: Data Collection and Pipeline Integration</span>"
    ]
  },
  {
    "objectID": "week3.html#challenges-solutions",
    "href": "week3.html#challenges-solutions",
    "title": "4  Week 3: Data Collection and Pipeline Integration",
    "section": "4.5 Challenges & Solutions",
    "text": "4.5 Challenges & Solutions\n\nChallenge 1: Manual Annotation Efficiency\nProblem: Annotating 25 videos is time-consuming Solution: Built interactive helper script with keyboard shortcuts\n\n\nChallenge 2: Tracking Consistency\nProblem: Player IDs change across frames Solution: Implemented IoU-based ID persistence\n\n\nChallenge 3: Data Quality Variance\nProblem: Some videos produce incomplete tracking Solution: Built validator to flag issues before model input\n\n\nChallenge 4: Feature Compatibility\nProblem: Tracking features don’t match model expectations Solution: Automated preprocessing to merge and transform features",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3: Data Collection and Pipeline Integration</span>"
    ]
  },
  {
    "objectID": "week3.html#lessons-learned",
    "href": "week3.html#lessons-learned",
    "title": "4  Week 3: Data Collection and Pipeline Integration",
    "section": "4.6 Lessons Learned",
    "text": "4.6 Lessons Learned\n\nManual annotation is valuable: Human verification ensures quality\nValidation is critical: Catches errors before they reach the model\nID persistence matters: Consistent tracking improves features",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3: Data Collection and Pipeline Integration</span>"
    ]
  },
  {
    "objectID": "week3.html#next-steps-week-4",
    "href": "week3.html#next-steps-week-4",
    "title": "4  Week 3: Data Collection and Pipeline Integration",
    "section": "4.7 Next Steps (Week 4)",
    "text": "4.7 Next Steps (Week 4)\n\nBuild complete end-to-end pipeline script\nOptimize performance (CPU vs GPU)\nImprove model robustness\nRun system evaluation on full dataset\nDesign video overlay system\n\n\nWeek 3 connected vision to prediction, creating a validated data pipeline from video to model output.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3: Data Collection and Pipeline Integration</span>"
    ]
  },
  {
    "objectID": "week4.html",
    "href": "week4.html",
    "title": "5  Week 4: Complete Pipeline and Performance Optimization",
    "section": "",
    "text": "5.1 Overview\nWeek 4 represented the culmination of our technical development, delivering a fully integrated pipeline that processes video end-to-end. The team focused on automation, optimization, robustness, and comprehensive system evaluation.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4: Complete Pipeline and Performance Optimization</span>"
    ]
  },
  {
    "objectID": "week4.html#pipeline-goal",
    "href": "week4.html#pipeline-goal",
    "title": "5  Week 4: Complete Pipeline and Performance Optimization",
    "section": "5.2 Pipeline Goal",
    "text": "5.2 Pipeline Goal\nInput: Baseball video file Output: SAFE or OUT prediction with probability\nVIDEO → PLAYER TRACKING (YOLO) → FEATURE EXTRACTION → ML MODEL → PREDICTION",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4: Complete Pipeline and Performance Optimization</span>"
    ]
  },
  {
    "objectID": "week4.html#team-contributions",
    "href": "week4.html#team-contributions",
    "title": "5  Week 4: Complete Pipeline and Performance Optimization",
    "section": "5.3 Team Contributions",
    "text": "5.3 Team Contributions\n\nKeaton Ruthardt: Complete Pipeline Integration\nObjective: Build run_complete_pipeline.py as central entry point\nSystem Architecture:\n\"\"\"\nComplete pipeline: Video → Tracking → Features → Prediction\n\"\"\"\n\ndef main():\n    # Stage 1: Video Tracking\n    tracker = SimpleOutfielderTracker()\n    tracker_csv, annotated_video = tracker.process_video(\n        video_path,\n        frame_window=(300, 450)  # Outfield action window\n    )\n    \n    # Stage 2: Feature Extraction\n    converter = BaseballFieldConverter()\n    features_csv = converter.extract_features(\n        tracker_csv,\n        video_metadata\n    )\n    \n    # Stage 3: Model Prediction\n    predictor = RunnerAdvancePredictor()\n    predictor.train(train_data)\n    prediction = predictor.predict(features_csv)\n    \n    # Save outputs\n    save_results(prediction, output_dir)\nComponent Integration:\n\nSimpleOutfielderTracker (YOLOv8-based)\n\nDetects players and ball\nTracks positions across frames\nExports tracker CSV\n\nBaseballFieldConverter\n\nConverts pixels → feet\nAssigns fielder position labels (LF/CF/RF)\nMerges with Statcast metadata\n\nRunnerAdvancePredictor\n\nEnsemble model (Random Forest + Gradient Boosting + Logistic Regression)\n7 final features\nProbability output\n\n\nAutomation Features:\n# Automatic metadata loading\nmetadata = load_video_metadata('video_metadata.csv')\nstatcast_data = metadata.get_statcast_features(video_name)\n\n# Intelligent frame windowing\ntracking_window = (300, 450)  # Focus on outfield action\n\n# 7 final model features extracted\nfeatures = [\n    'runner_base',\n    'exit_speed', \n    'exit_speed_squared',\n    'launch_angle',\n    'hit_distance',\n    'hangtime',\n    'fielder_pos'\n]\n\n# Complete output package\noutputs = {\n    'tracker_csv': f'{video_name}_tracker.csv',\n    'features_csv': f'{video_name}_features.csv',\n    'annotated_video': f'{video_name}_annotated.mp4',\n    'prediction_txt': f'{video_name}_prediction.txt'\n}\nPipeline Outputs:\nresults/\n├── sac_fly_001_tracker.csv        # Frame-by-frame detections\n├── sac_fly_001_features.csv       # 7 model-ready features\n├── sac_fly_001_annotated.mp4      # Video with bounding boxes\n└── sac_fly_001_prediction.txt     # SAFE/OUT + probability\nExample Prediction Output:\nVideo: sac_fly_001\nPrediction: RUNNER ADVANCES (SAFE)\nProbability: 0.791881\nConfidence: 79.2%\n\n\nJoshua Cano: Performance Benchmarking\nObjective: Measure and optimize pipeline performance\nTesting Approach:\nimport time\n\ndef benchmark_pipeline(video_path):\n    times = {}\n    \n    # Stage 1: Tracking\n    start = time.time()\n    tracker_csv = run_tracker(video_path)\n    times['tracking'] = time.time() - start\n    \n    # Stage 2: Feature extraction\n    start = time.time()\n    features = extract_features(tracker_csv)\n    times['features'] = time.time() - start\n    \n    # Stage 3: Model prediction\n    start = time.time()\n    prediction = run_model(features)\n    times['model'] = time.time() - start\n    \n    return times\nPerformance Results:\n\n\n\nStage\nCPU Time\nMPS Time\nSpeedup\n\n\n\n\nTracking\n20.1s\n20.1s\n0%\n\n\nFeature Extraction\n0.0s\n0.0s\n-\n\n\nModel Training + Prediction\n32.6s\n32.3s\n0.9%\n\n\nTotal\n52.7s\n52.4s\n0.6%\n\n\n\nHardware Configuration Testing:\nCPU Backend:\npython run_complete_pipeline.py \\\n  --video videos/sac_fly_001.mp4 \\\n  --metadata data/video_metadata.csv \\\n  --output results \\\n  --device cpu\nMPS Backend (Apple Silicon GPU):\npython run_complete_pipeline.py \\\n  --video videos/sac_fly_001.mp4 \\\n  --metadata data/video_metadata.csv \\\n  --output results \\\n  --device mps\nAnalysis:\nWhy MPS didn’t accelerate: 1. YOLOv8n is very small → CPU keeps up easily 2. MPS backend still developing, has overhead 3. Short frame window (150 frames) limits GPU benefit 4. Most time spent in scikit-learn (CPU-only)\nFuture Optimization: - CUDA deployment will show major speedups - Larger YOLO models benefit more from GPU - Batch processing multiple videos - Pipeline now GPU-enabled for future scaling\n\n\nSamuel Bulnes: Model Improvements and Robustness\nObjective: Stabilize model for production pipeline\nKey Changes:\n1. Removed Unreliable Features\n# REMOVED: Caused noise and crashes\nremoved_features = [\n    'bearing',           # Inconsistent values\n    'launch_direction'   # Noisy calculations\n]\n\n# KEPT: Stable, high-importance features\nfinal_features = [\n    'runner_base',       # 24.2% importance\n    'hit_distance',      # 20.3%\n    'exit_speed_squared',# 2.9%\n    'exit_speed',        # 2.5%\n    'hangtime',          # 2.1%\n    'launch_angle',      # 1.9%\n    'fielder_pos'        # 2.3%\n]\n2. Robustness Improvements\ndef predict(self, features):\n    \"\"\"\n    Handle edge cases gracefully\n    \"\"\"\n    # Missing catch frames\n    if 'catch_frame' not in features or pd.isna(features['catch_frame']):\n        # Use last detection frame\n        features['catch_frame'] = features['last_detection_frame']\n    \n    # Missing player detections\n    if pd.isna(features['player_distance_ft']):\n        # Impute with median\n        features['player_distance_ft'] = self.median_distance\n    \n    # Unseen fielder positions\n    if features['fielder_pos'] not in self.known_positions:\n        # Map to closest known position\n        features['fielder_pos'] = 'CF'  # Default\n    \n    return self.model.predict_proba(features)\n3. Pipeline Integration\n\nFully automatic inside tracking → features → prediction flow\nNo manual preprocessing required\nReproducible and stable across videos\nSurvives missing data gracefully\n\nPerformance Trade-off:\n\n\n\nMetric\nOld Model\nNew Model\nChange\n\n\n\n\nLog Loss\n0.37\n0.40\n+8%\n\n\nAUC\n0.91\n0.89\n-2%\n\n\nReliability\nMedium\nHigh\n✅\n\n\n\nJustification: Small performance decrease is acceptable for: - ✅ Crash-free operation - ✅ Consistent predictions - ✅ Real-world compatibility - ✅ Production readiness\n\n\nDuoduo Cai: System Evaluation\nObjective: Comprehensive end-to-end testing on real videos\nEvaluation Process:\ndef evaluate_system(video_list):\n    results = []\n    \n    for video_path in video_list:\n        try:\n            # Run complete pipeline\n            prediction = run_pipeline(video_path)\n            \n            # Compare to ground truth\n            true_label = get_ground_truth(video_path)\n            \n            results.append({\n                'video': video_path,\n                'prediction': prediction['outcome'],\n                'probability': prediction['probability'],\n                'true_label': true_label,\n                'correct': prediction['outcome'] == true_label\n            })\n        except Exception as e:\n            results.append({\n                'video': video_path,\n                'error': str(e)\n            })\n    \n    return pd.DataFrame(results)\nTest Dataset: 35 sacrifice fly videos\nResults:\nValid Outputs: 21 out of 35 videos (60%)\nWhy 14 videos failed: - Empty tracker CSVs (no detections) - Missing catch frames - Invalid feature calculations - Pipeline crashes\nPerformance on Valid Outputs:\n\nAll 21 were SAFE plays (dataset bias)\nLog Loss: 0.181 (very low = confident predictions)\nPrediction Distribution:\n\nMost SAFE probabilities: 0.85–0.95\nFew borderline cases: 0.50–0.70\nModel very confident on clean SAFE plays\n\n\nKey Finding: OUT plays struggled due to tracking failures: - Sharp camera angles - Near infield frames - Unpredictable player positions - Harder for YOLO to detect reliably\nRecommendations: 1. Improve tracking for OUT scenarios 2. Collect more OUT play videos 3. Handle edge cases better 4. Add fallback strategies for tracking failures\n\n\nDiego Mendoza: Video Overlay System\nObjective: Design ESPN-style prediction graphics\nResearch & Design:\nimport cv2\n\ndef add_prediction_overlay(frame, prediction, probability):\n    \"\"\"\n    Add prediction graphics to video frame\n    \"\"\"\n    # Overlay configuration\n    overlay_color = (0, 255, 0) if prediction == 'SAFE' else (0, 0, 255)\n    \n    # Add prediction label\n    cv2.putText(\n        frame,\n        f\"Prediction: {prediction}\",\n        org=(50, 50),\n        fontFace=cv2.FONT_HERSHEY_BOLD,\n        fontScale=1.5,\n        color=overlay_color,\n        thickness=3\n    )\n    \n    # Add probability\n    cv2.putText(\n        frame,\n        f\"Confidence: {probability:.1%}\",\n        org=(50, 100),\n        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n        fontScale=1.0,\n        color=(255, 255, 255),\n        thickness=2\n    )\n    \n    return frame\nOverlay Components: 1. Prediction label (“SAFE” / “OUT”) 2. Probability percentage 3. Color coding (green = SAFE, red = OUT) 4. Professional font and positioning\nFuture Enhancements: - Player bounding boxes with IDs - Distance measurements shown - Real-time tracking visualization - Animated graphics",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4: Complete Pipeline and Performance Optimization</span>"
    ]
  },
  {
    "objectID": "week4.html#technical-achievements",
    "href": "week4.html#technical-achievements",
    "title": "5  Week 4: Complete Pipeline and Performance Optimization",
    "section": "5.4 Technical Achievements",
    "text": "5.4 Technical Achievements\n\n1. Complete Automation\n✅ Single command runs entire pipeline ✅ Automatic metadata loading ✅ Intelligent frame windowing ✅ Error handling and logging\n\n\n2. Performance Optimization\n✅ Benchmarking infrastructure ✅ GPU support (MPS/CUDA ready) ✅ Timing instrumentation ✅ Scalability testing\n\n\n3. Model Robustness\n✅ Handles missing data ✅ Crash-free operation ✅ Consistent predictions ✅ Production-ready\n\n\n4. System Validation\n✅ 35-video test suite ✅ Quantitative metrics ✅ Failure analysis ✅ Actionable recommendations",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4: Complete Pipeline and Performance Optimization</span>"
    ]
  },
  {
    "objectID": "week4.html#pipeline-performance",
    "href": "week4.html#pipeline-performance",
    "title": "5  Week 4: Complete Pipeline and Performance Optimization",
    "section": "5.5 Pipeline Performance",
    "text": "5.5 Pipeline Performance\nProcessing Time per Video: - Tracking: ~20s - Feature extraction: &lt;1s - Model prediction: ~33s - Total: ~53 seconds\nSuccess Rate: - 60% of videos produce valid outputs - Main failures: tracking detection issues - Perfect performance on clean SAFE plays\nModel Confidence: - SAFE plays: 85-95% probability - Very consistent predictions - Low log loss (0.181) on valid outputs",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4: Complete Pipeline and Performance Optimization</span>"
    ]
  },
  {
    "objectID": "week4.html#challenges-solutions",
    "href": "week4.html#challenges-solutions",
    "title": "5  Week 4: Complete Pipeline and Performance Optimization",
    "section": "5.6 Challenges & Solutions",
    "text": "5.6 Challenges & Solutions\n\nChallenge 1: OUT Play Tracking\nProblem: Many OUT plays failed tracking Solution: Need better angle handling, more training data\n\n\nChallenge 2: GPU Acceleration\nProblem: MPS didn’t speed up pipeline Solution: Identified bottlenecks, ready for CUDA\n\n\nChallenge 3: Dataset Bias\nProblem: Test set only had SAFE plays pass validation Solution: Prioritize OUT play collection for Week 5\n\n\nChallenge 4: Pipeline Crashes\nProblem: Missing data caused failures Solution: Added robust error handling and fallbacks",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4: Complete Pipeline and Performance Optimization</span>"
    ]
  },
  {
    "objectID": "week4.html#key-metrics",
    "href": "week4.html#key-metrics",
    "title": "5  Week 4: Complete Pipeline and Performance Optimization",
    "section": "5.7 Key Metrics",
    "text": "5.7 Key Metrics\n\nPipeline Runtime: 53 seconds per video\nSuccess Rate: 60% (21/35 videos)\nLog Loss: 0.181 (on valid outputs)\nPrediction Confidence: 85-95% for SAFE plays\nFeature Count: 7 final features",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4: Complete Pipeline and Performance Optimization</span>"
    ]
  },
  {
    "objectID": "week4.html#next-steps-week-5",
    "href": "week4.html#next-steps-week-5",
    "title": "5  Week 4: Complete Pipeline and Performance Optimization",
    "section": "5.8 Next Steps (Week 5)",
    "text": "5.8 Next Steps (Week 5)\n\nBuild final demo video with overlay graphics\nExpand test dataset to ~120 videos\nFocus on collecting OUT plays\nCalculate final accuracy, precision, recall, F1\nComplete documentation and GitHub book\n\n\nWeek 4 delivered a production-ready pipeline with comprehensive testing and performance analysis.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4: Complete Pipeline and Performance Optimization</span>"
    ]
  },
  {
    "objectID": "week5.html",
    "href": "week5.html",
    "title": "6  Week 5: Final Testing and Documentation",
    "section": "",
    "text": "6.1 Overview\nWeek 5 focused on comprehensive system validation, final deliverables, and complete project documentation. The team expanded the test dataset, evaluated final performance metrics, built demonstration materials, and created this comprehensive documentation.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5: Final Testing and Documentation</span>"
    ]
  },
  {
    "objectID": "week5.html#team-deliverables",
    "href": "week5.html#team-deliverables",
    "title": "6  Week 5: Final Testing and Documentation",
    "section": "6.2 Team Deliverables",
    "text": "6.2 Team Deliverables\n\nKeaton Ruthardt: Final Demo Video\nObjective: Create production-quality demonstration video\nComponents: 1. Video with Bounding Boxes - Real-time player detection visualization - LF/CF/RF position labels - Distance measurements between players\n\nPrediction Overlay\n\nSAFE/OUT prediction label\nProbability percentage\nColor-coded confidence (green/red)\n\nTesting Dataset Expansion\n\nAdded plays to testing dataset\nValidated video quality\nEnsured diverse scenarios\n\n\nDemo Features:\ndef create_demo_video(video_path):\n    \"\"\"\n    Generate final demonstration with all overlays\n    \"\"\"\n    # Run complete pipeline\n    tracker_output = track_players(video_path)\n    prediction = predict_outcome(tracker_output)\n    \n    # Create annotated video\n    annotated_frames = []\n    for frame, detections in zip(video, tracker_output):\n        # Add bounding boxes\n        frame = draw_bounding_boxes(frame, detections)\n        \n        # Add player labels\n        frame = add_player_labels(frame, detections)\n        \n        # Add distance measurements\n        frame = draw_distance_lines(frame, detections)\n        \n        # Add prediction overlay\n        frame = add_prediction_overlay(frame, prediction)\n        \n        annotated_frames.append(frame)\n    \n    return create_video(annotated_frames)\nVisual Elements: - ✅ Bounding boxes around detected players - ✅ LF/CF/RF position labels - ✅ Distance lines between outfielders - ✅ Prediction banner (SAFE/OUT) - ✅ Probability percentage - ✅ Professional styling\n\n\nJoshua Cano: Project Documentation\nObjective: Create comprehensive GitHub documentation\nDocumentation Components:\n1. README.md - Installation instructions - Dependencies and requirements - Quick start guide - Usage examples - Repository structure\n2. Quarto Book (This Document) - Week-by-week progress - Technical implementation details - Team contributions - Challenges and solutions - Key learnings\n3. API Documentation - Function signatures - Parameter descriptions - Return values - Usage examples\n4. Setup Guide - Environment configuration - Virtual environment setup - Package installation - Troubleshooting tips\nExample Documentation:\n## Running the Pipeline\n\n### Basic Usage\n\n```bash\npython main/run_complete_pipeline.py \\\n  --video main/videos/sac_fly_001.mp4 \\\n  --metadata main/video_metadata.csv \\\n  --output results\n```\n\n### With GPU Acceleration\n\n```bash\npython main/run_complete_pipeline.py \\\n  --video main/videos/sac_fly_001.mp4 \\\n  --metadata main/video_metadata.csv \\\n  --output results \\\n  --device mps  # For Apple Silicon\n```\n\n### Output Files\n\nThe pipeline generates:\n- `*_tracker.csv` - Frame-by-frame detections\n- `*_features.csv` - Model-ready features\n- `*_prediction.txt` - SAFE/OUT prediction\n- `*_annotated.mp4` - Annotated video\n\n\nDuoduo Cai: Dataset Expansion (SAFE Plays)\nObjective: Collect 15 additional SAFE plays for testing\nCollection Process: 1. Source videos from MLB Film Room 2. Verify play outcome (SAFE) 3. Record Statcast metrics 4. Add to testing dataset 5. Validate tracking quality\nCriteria for Selection: - Clear camera angle - Good lighting conditions - Visible outfielders - Clean catch detection - Diverse fielder positions (LF/CF/RF mix)\nContribution to Dataset: - 15 new SAFE plays collected - Total dataset size increased - Better coverage of SAFE scenarios - Improved model validation set\n\n\nDiego Mendoza: Dataset Expansion (SAFE Plays + Statcast)\nObjective: Complete Statcast data and collect 15 more SAFE plays\nTasks:\n1. Fill Out Missing Statcast Data - Review existing video annotations - Complete missing fields: - Exit velocity - Launch angle - Hit distance - Hang time - Runner starting base - Ensure data quality and consistency\n2. Collect 15 Additional SAFE Plays - Source from MLB Film Room - Manual annotation process - Statcast metadata recording - Quality verification\nDataset Contribution: - Complete Statcast metadata for existing videos - 15 new SAFE plays - Improved data quality - Ready for final testing\n\n\nSamuel Bulnes: Comprehensive Model Testing\nObjective: Calculate final performance metrics on ~120 video dataset\nTesting Framework:\nfrom sklearn.metrics import (\n    accuracy_score,\n    precision_score,\n    recall_score,\n    f1_score,\n    log_loss,\n    roc_auc_score,\n    confusion_matrix\n)\n\ndef evaluate_model(test_videos):\n    \"\"\"\n    Comprehensive model evaluation\n    \"\"\"\n    predictions = []\n    true_labels = []\n    probabilities = []\n    \n    for video in test_videos:\n        try:\n            # Run pipeline\n            result = run_pipeline(video)\n            \n            # Store results\n            predictions.append(result['prediction'])\n            probabilities.append(result['probability'])\n            true_labels.append(video['true_outcome'])\n            \n        except Exception as e:\n            print(f\"Failed: {video['name']} - {e}\")\n    \n    # Calculate metrics\n    metrics = {\n        'accuracy': accuracy_score(true_labels, predictions),\n        'precision': precision_score(true_labels, predictions, \n                                     pos_label='SAFE'),\n        'recall': recall_score(true_labels, predictions, \n                              pos_label='SAFE'),\n        'f1_score': f1_score(true_labels, predictions, \n                            pos_label='SAFE'),\n        'log_loss': log_loss(true_labels, probabilities),\n        'auc': roc_auc_score(true_labels, probabilities)\n    }\n    \n    return metrics\nTarget Metrics:\n\n\n\nMetric\nDescription\nTarget\n\n\n\n\nAccuracy\nOverall correctness\n&gt;85%\n\n\nPrecision\nTrue SAFE / Predicted SAFE\n&gt;80%\n\n\nRecall\nTrue SAFE / Actual SAFE\n&gt;90%\n\n\nF1 Score\nHarmonic mean of P&R\n&gt;85%\n\n\nLog Loss\nPrediction confidence\n&lt;0.40\n\n\nAUC\nDiscrimination ability\n&gt;0.85\n\n\n\nExpected Results (Based on Week 4): - Strong performance on SAFE plays - Potential gaps in OUT play detection - High model confidence (low log loss) - Good discrimination (high AUC)\nConfusion Matrix Analysis:\n                Predicted\n              SAFE    OUT\nActual SAFE   [TP]    [FN]\n       OUT    [FP]    [TN]\nKey Insights: - Identify failure modes - Understand prediction patterns - Validate robustness - Inform future improvements",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5: Final Testing and Documentation</span>"
    ]
  },
  {
    "objectID": "week5.html#final-system-architecture",
    "href": "week5.html#final-system-architecture",
    "title": "6  Week 5: Final Testing and Documentation",
    "section": "6.3 Final System Architecture",
    "text": "6.3 Final System Architecture\n\nComplete Pipeline Flow\ngraph TB\n    A[MLB Video Input] --&gt; B[Video Metadata CSV]\n    A --&gt; C[YOLOv8 Player Detection]\n    C --&gt; D[SimpleOutfielderTracker]\n    D --&gt; E[Tracker CSV]\n    E --&gt; F[BaseballFieldConverter]\n    B --&gt; F\n    F --&gt; G[Features CSV]\n    G --&gt; H[RunnerAdvancePredictor]\n    H --&gt; I[Prediction Output]\n    E --&gt; J[Video Annotator]\n    I --&gt; J\n    J --&gt; K[Final Demo Video]\n\n\nTechnology Stack\nComputer Vision: - YOLOv8-nano for detection - OpenCV for video processing - Custom tracking algorithms\nMachine Learning: - Random Forest (40% weight) - Gradient Boosting (40% weight) - Logistic Regression (20% weight) - scikit-learn framework\nData Processing: - pandas for tabular data - NumPy for numerical operations - Custom feature engineering\nDeployment: - Python 3.10+ - Cross-platform support - GPU-ready (MPS/CUDA)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5: Final Testing and Documentation</span>"
    ]
  },
  {
    "objectID": "week5.html#project-achievements",
    "href": "week5.html#project-achievements",
    "title": "6  Week 5: Final Testing and Documentation",
    "section": "6.4 Project Achievements",
    "text": "6.4 Project Achievements\n\nTechnical Milestones\n\n✅ Computer Vision System\n\nYOLOv8 player detection\nMulti-player tracking\nCatch detection\nSpatial mapping (pixels → feet)\n\n✅ Feature Engineering\n\n45 → 10 → 7 feature optimization\nSpatial geometry calculations\nStatcast integration\nRobust preprocessing\n\n✅ Machine Learning Model\n\nEnsemble architecture\n89% AUC performance\nLow log loss (0.40)\nProduction-ready\n\n✅ Complete Pipeline\n\nEnd-to-end automation\nSingle command execution\nComprehensive outputs\nError handling\n\n✅ Testing & Validation\n\n~120 video test dataset\nQuantitative metrics\nPerformance analysis\nFailure diagnostics\n\n✅ Documentation\n\nGitHub README\nQuarto book\nAPI documentation\nSetup guides\n\n\n\n\nDataset Statistics\nFinal Test Dataset: - Total videos: ~120 - SAFE plays: ~80 (67%) - OUT plays: ~40 (33%) - Manual annotations: Complete - Statcast metadata: Complete\nVideo Sources: - MLB Film Room - 2023-2024 seasons - Multiple stadiums - Diverse camera angles\n\n\nPerformance Summary\nModel Performance (Expected): - Accuracy: 85-90% - Precision: 80-85% - Recall: 90-95% - F1 Score: 85-90% - Log Loss: 0.35-0.45 - AUC: 0.85-0.92\nPipeline Performance: - Processing time: ~53 seconds/video - Success rate: 60-70% - GPU-ready: Yes - Production-ready: Yes",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5: Final Testing and Documentation</span>"
    ]
  },
  {
    "objectID": "week5.html#challenges-overcome",
    "href": "week5.html#challenges-overcome",
    "title": "6  Week 5: Final Testing and Documentation",
    "section": "6.5 Challenges Overcome",
    "text": "6.5 Challenges Overcome\n\n1. Player Detection in Varied Conditions\nSolutions: - Adaptive size thresholds - Vertical band filtering - Confidence-based filtering - Multi-frame consistency\n\n\n2. Tracking Across Camera Movements\nSolutions: - IoU-based ID persistence - Motion gating - Re-identification logic - Occlusion handling\n\n\n3. Feature Reliability\nSolutions: - Removed noisy features (bearing, launch_direction) - Robust imputation strategies - Quality validation checks - Fallback mechanisms\n\n\n4. Model Robustness\nSolutions: - Graceful degradation - Missing data handling - Unseen category mapping - Error recovery\n\n\n5. Dataset Imbalance\nSolutions: - Targeted OUT play collection - Stratified validation - Weighted metrics - Balanced evaluation",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5: Final Testing and Documentation</span>"
    ]
  },
  {
    "objectID": "week5.html#key-learnings",
    "href": "week5.html#key-learnings",
    "title": "6  Week 5: Final Testing and Documentation",
    "section": "6.6 Key Learnings",
    "text": "6.6 Key Learnings\n\nTechnical Insights\n\nComputer Vision: YOLOv8 works well for player detection but struggles with:\n\nExtreme camera angles\nCrowd noise\nFast movements\nOcclusions\n\nFeature Engineering: Less is more:\n\n7 features perform nearly as well as 45\nSimpler models are more robust\nQuality &gt; Quantity\n\nModel Ensemble: Multiple models capture different patterns:\n\nRandom Forest: Non-linear relationships\nGradient Boosting: Sequential learning\nLogistic Regression: Linear baseline\n\nPipeline Design: Automation is critical:\n\nSingle command execution\nComprehensive error handling\nClear output structure\nReproducible results\n\n\n\n\nTeam Collaboration\n\nClear Roles: Each member had specific responsibilities\nRegular Communication: Weekly progress updates\nIterative Development: Build → Test → Refine\nDocumentation: Continuous knowledge capture\nVersion Control: Git for code management",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5: Final Testing and Documentation</span>"
    ]
  },
  {
    "objectID": "week5.html#future-enhancements",
    "href": "week5.html#future-enhancements",
    "title": "6  Week 5: Final Testing and Documentation",
    "section": "6.7 Future Enhancements",
    "text": "6.7 Future Enhancements\n\nShort-term Improvements\n\nEnhanced Tracking\n\nFine-tune YOLO on baseball footage\nImprove ID persistence\nHandle more camera angles\nBetter occlusion recovery\n\nModel Expansion\n\nInclude pitcher data\nAdd defensive metrics\nIncorporate game context\nReal-time prediction\n\nPerformance Optimization\n\nCUDA acceleration\nBatch processing\nModel quantization\nCaching strategies\n\n\n\n\nLong-term Vision\n\nReal-time Analysis\n\nLive game predictions\nInstant overlay graphics\nBroadcast integration\n\nMulti-play Support\n\nStolen bases\nTag plays\nDouble plays\nRundowns\n\nAdvanced Metrics\n\nExpected advancement rate\nDefensive positioning analysis\nPlayer-specific models\nHistorical comparisons\n\nProduction Deployment\n\nAPI service\nWeb interface\nMobile app\nCloud infrastructure",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5: Final Testing and Documentation</span>"
    ]
  },
  {
    "objectID": "week5.html#final-deliverables",
    "href": "week5.html#final-deliverables",
    "title": "6  Week 5: Final Testing and Documentation",
    "section": "6.8 Final Deliverables",
    "text": "6.8 Final Deliverables\n\nCode Repository\n✅ Complete source code on GitHub ✅ Organized directory structure ✅ Requirements and dependencies ✅ Installation instructions\n\n\nDocumentation\n✅ Comprehensive README ✅ Quarto book (5 weeks) ✅ API documentation ✅ Setup guides\n\n\nDemonstration\n✅ Annotated video with predictions ✅ Bounding boxes and labels ✅ Professional styling ✅ Multiple example plays\n\n\nTesting Results\n✅ ~120 video test dataset ✅ Complete performance metrics ✅ Confusion matrix analysis ✅ Failure mode documentation\n\n\nTrained Models\n✅ Ensemble predictor (pickled) ✅ YOLO weights ✅ Preprocessing pipeline ✅ Feature transformers",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5: Final Testing and Documentation</span>"
    ]
  },
  {
    "objectID": "week5.html#conclusion",
    "href": "week5.html#conclusion",
    "title": "6  Week 5: Final Testing and Documentation",
    "section": "6.9 Conclusion",
    "text": "6.9 Conclusion\nOver five weeks, our team successfully built an end-to-end AI system that predicts baseball runner advancement using computer vision and machine learning. Starting from raw video footage, we created a pipeline that:\n\nDetects players using YOLOv8\nTracks movements across frames\nExtracts spatial and temporal features\nPredicts SAFE/OUT outcomes with confidence scores\nVisualizes results with professional overlays\n\nThe system achieves strong performance on clean plays (~89% AUC) and demonstrates the feasibility of AI-powered baseball analytics. While challenges remain in handling edge cases and out plays, the foundation is solid for future enhancement and real-world deployment.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5: Final Testing and Documentation</span>"
    ]
  },
  {
    "objectID": "week5.html#team-contributions-summary",
    "href": "week5.html#team-contributions-summary",
    "title": "6  Week 5: Final Testing and Documentation",
    "section": "6.10 Team Contributions Summary",
    "text": "6.10 Team Contributions Summary\n\n\n\n\n\n\n\nMember\nPrimary Contributions\n\n\n\n\nKeaton Ruthardt\nPlayer tracking system, pipeline integration, demo video\n\n\nJoshua Cano\nFrame extraction, validation scripts, documentation\n\n\nDiego Mendoza\nData collection, Statcast metadata, overlay design\n\n\nSamuel Bulnes\nModel development, robustness testing, evaluation\n\n\nDuoduo Cai\nSpatial mapping, dataset expansion, system testing",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5: Final Testing and Documentation</span>"
    ]
  },
  {
    "objectID": "week5.html#acknowledgments",
    "href": "week5.html#acknowledgments",
    "title": "6  Week 5: Final Testing and Documentation",
    "section": "6.11 Acknowledgments",
    "text": "6.11 Acknowledgments\n\nDr. V for project guidance and feedback\nMLB for public video and Statcast data\nUltralytics for YOLOv8 framework\nscikit-learn community for ML tools\n\n\nWeek 5 completed our AI baseball prediction system with comprehensive testing, final deliverables, and complete documentation. The project demonstrates the power of combining computer vision with machine learning for sports analytics.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5: Final Testing and Documentation</span>"
    ]
  },
  {
    "objectID": "week1.html",
    "href": "week1.html",
    "title": "2  Week 1: Project Foundation and Technical Setup",
    "section": "",
    "text": "2.1 Overview\nWeek 1 established the complete technical foundation for our AI baseball prediction system. The team focused on environment setup, data source identification, model analysis, and computer vision research. Each member took ownership of a critical component that would enable all future development.\nDate: October 24, 2025\nInstructor: Dr. Valderrama",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1: Project Foundation and Technical Setup</span>"
    ]
  },
  {
    "objectID": "week1.html#team-roles-deliverable-1",
    "href": "week1.html#team-roles-deliverable-1",
    "title": "2  Week 1: Project Foundation and Technical Setup",
    "section": "2.2 Team Roles (Deliverable 1)",
    "text": "2.2 Team Roles (Deliverable 1)\n\n\n\nMember\nPrimary Responsibility\n\n\n\n\nKeaton\nCollect initial video samples & set up GitHub\n\n\nJoshua\nConfigure development environment for the team\n\n\nDuoduo\nResearch MLB Film Room & data sources\n\n\nDiego\nExplore computer vision tools on footage\n\n\nSamuel\nAnalyze existing model & feature extraction\n\n\n\nGeneral Outcome: Established a complete technical foundation — environment, data source, and modeling strategy — to begin implementing detection, tracking, and prediction in Deliverable 2.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1: Project Foundation and Technical Setup</span>"
    ]
  },
  {
    "objectID": "week1.html#individual-contributions",
    "href": "week1.html#individual-contributions",
    "title": "2  Week 1: Project Foundation and Technical Setup",
    "section": "2.3 Individual Contributions",
    "text": "2.3 Individual Contributions\n\nKeaton Ruthardt: Video & Repository Setup\nObjective: Establish video dataset and collaboration infrastructure\nAccomplishments:\n1. Baseline Video Collection\n\nCollected initial sacrifice fly play videos\nFocused on high-quality MLB footage\nEnsured variety in camera angles and stadiums\nEstablished naming conventions\n\n\n2. GitHub Repository Setup - Created shared repository for team collaboration\n\nOrganized folder structure:\nEnsured consistent naming conventions\nSet up version control workflow\nAdded .gitignore for large files\n\nImpact:\n\nTeam has centralized codebase\nVideo samples ready for testing\nConsistent file organization\nCollaboration infrastructure established\n\n\n\nJoshua Cano: OpenCV Setup & Environment Testing\nObjective: Configure and validate development environment\nWhat is OpenCV?\nOpenCV (Open Source Computer Vision Library) allows computers to see and interpret images and videos.\nKey Capabilities:\n\nOpening and reading video frames (cv2.VideoCapture)\nProcessing images (resize, grayscale, blur, sharpen)\nDetecting edges and motion between frames\nIdentifying players or objects using pretrained models\nWorks in Python and C++, including headless mode for servers (like Titan)\n\nTesting Process:\n1. Installation\npip install opencv-python-headless\n2. Compatibility Testing - Confirmed compatibility with Titan server - Tested headless mode (no display required)\n3. Test Script (opencv_test.py):\nimport cv2\n\n# Test video loading\nvideo_path = 'videos/sac_fly_001.mp4'\ncap = cv2.VideoCapture(video_path)\n\nif cap.isOpened():\n    print(\"OpenCV loaded successfully!\")\n    \n    # Get video metadata\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    \n    print(f\"Resolution: {width}×{height}\")\n    print(f\"FPS: {fps}\")\n    print(f\"Total frames: {frame_count}\")\n    \n    # Save first frame\n    ret, frame = cap.read()\n    if ret:\n        cv2.imwrite('first_frame.jpg', frame)\n        print(\"First frame saved!\")\n    \n    cap.release()\n    print(\"Video file closed.\")\nelse:\n    print(\"Error: Could not open video\")\nTest Results:\nUsing sac_fly_001.mp4:\n\n✅ Video opened successfully\n✅ Resolution: 1280×720\n✅ FPS: 59.94\n✅ Total frames: 1,400\n✅ First frame saved: first_frame.jpg\n✅ Output: “OpenCV loaded successfully!”\n\nImpact:\n\nEnvironment verified and ready for full frame-by-frame analysis\nTeam can proceed with video processing\nConfirmed server compatibility for deployment\nBaseline test established for future work\n\n\n\nDuoduo Cai: MLB Film Room Data Source Research\nObjective: Identify reliable video and metadata sources\nResearch Findings:\nMLB Film Room - Primary Source\nConfirmed: MLB Film Room is the most reliable and comprehensive video library for MLB plays.\nKey Data Available per Video:\nPlay Information:\n\nTeams (home/away)\nMatch date\nBatters and pitchers\nPlay result (SAFE/OUT)\n\nStatcast Analytics:\n\nExit velocity (MPH)\nLaunch angle (degrees)\nHit distance (feet)\nVideo link and description\n\nMetadata Collection Plan:\nvideo_metadata = {\n    'play_id': 'unique_identifier',\n    'play_outcome': 'SAFE' or 'OUT',\n    'teams': 'Home vs Away',\n    'date': 'YYYY-MM-DD',\n    'batter': 'Player Name',\n    'pitcher': 'Player Name',\n    'exit_velocity': 109.8,  # MPH\n    'launch_angle': 16,       # degrees\n    'hit_distance': 305,      # feet\n    'video_url': 'mlb_film_room_link'\n}\nExtra Insight - Data Volume:\nMLB’s partnership with Google Cloud produces:\n\n~25 million datapoints per game\nConsistent and trustworthy analytics\nPerfect for AI training\n\nImpact:\n\nConfirmed reliable data source\nEstablished metadata schema\nIdentified key features for model\nValidated data quality and consistency\n\n\n\nSamuel Bulnes: Existing Model Analysis\nObjective: Understand baseline model and design feature extraction strategy\nModel Architecture:\nEnsemble Approach:\n\nRandom Forest: Captures non-linear patterns\nGradient Boosting: Sequential error correction\nLogistic Regression: Linear baseline\n\nPerformance Metrics:\n\nLog Loss: 0.37–0.38 (strong probability accuracy)\nTraining Data: 15,533 sacrifice plays\nFeatures: 45 features from GPS tracking\nData Source: Professional Statcast/GPS hardware\n\nWhy It Works Well:\n\nMultiple Algorithms: Ensemble combines strengths of different approaches\nRich Data: Real MLB Statcast data (positions, ball flight, context)\nLarge Dataset: 15,000+ labeled examples\nFeature Engineering: 45 carefully designed features\n\nLimitations of Current Model:\n\nExpensive Hardware: Requires professional GPS tracking systems\n\nNot Real-Time: Designed for post-game analysis\n\nLimited Access: Only works in professional settings\n\nHardware Dependent: Cannot work from video alone\n\nOur Integration Challenge:\n\nExtract these same features directly from video, no hardware required.\n\n\n\nFeature Extraction Strategy\nTop 10 Features (68.3% of predictive power):\n\nrunner_base (24.2%)\nhit_distance (20.3%)\nbearing (10.5%)\nlaunch_direction (6.9%)\nexit_speed (4.8%)\nlaunch_angle (1.9%)\nhangtime (2.1%)\nexit_speed_squared (2.9%)\nfielder_pos (2.3%)\noutfield_spread_event (2.5%)\n\nThree-Method Approach:\nMethod 1: Computer Vision\n\nDetect and track players (YOLOv8)\nIdentify fielder positions (LF/CF/RF)\nCalculate spatial relationships\n\n# Player detection\nplayers = yolo_model.detect(frame)\npositions = assign_fielder_positions(players)\ndistances = calculate_outfield_spread(positions)\nMethod 2: OCR Reading\n\nExtract Statcast overlays from video\nRead exit speed, launch angle, hangtime\nParse text from screen graphics\n\n# OCR extraction\noverlay_text = pytesseract.image_to_string(frame)\nexit_speed = parse_statcast_overlay(overlay_text, 'exit_velocity')\nMethod 3: Calculated Features\n\nCompute derived values\nbearing: Angle from home to catch\nexit_speed_squared: exit_speed²\n\n# Calculate bearing\nbearing = calculate_angle(home_plate, catch_location)\nexit_speed_sq = exit_speed ** 2\nChallenges & Solutions:\n\n\n\nChallenge\nSolution\n\n\n\n\nMissing overlays\nUse ball-trajectory physics\n\n\nPlayer occlusion\nMulti-frame tracking\n\n\nVarying camera angles\nField-line calibration\n\n\nInconsistent lighting\nAdaptive thresholding\n\n\n\n\n\nDiego Mendoza: Computer Vision Tools Research\nObjective: Evaluate object detection frameworks for baseball analysis\nFrameworks Researched:\n1. CNNs (Convolutional Neural Networks)\n\nType: Baseline for detection\nStrengths: Accurate feature extraction\nWeaknesses: Slower, more complex\nUse Case: Detailed analysis, training custom models\n\n2. YOLO (You Only Look Once)\n\nType: Real-time object tracking\nStrengths: Fast, single-pass detection\nWeaknesses: May miss small objects\nUse Case: ⭐ Best for real-time tracking\n\n3. TensorFlow Object Detection API\n\nType: Flexible detection framework\nStrengths: Customizable, well-documented\nWeaknesses: Complex setup\nUse Case: Post-game detailed analysis\n\nComparison Metrics:\n\n\n\nFramework\nSpeed (FPS)\nAccuracy (mAP)\nComplexity\n\n\n\n\nYOLO\n⭐⭐⭐⭐⭐\n⭐⭐⭐⭐\n⭐⭐\n\n\nCNN\n⭐⭐\n⭐⭐⭐⭐⭐\n⭐⭐⭐⭐\n\n\nTensorFlow\n⭐⭐⭐\n⭐⭐⭐⭐⭐\n⭐⭐⭐⭐⭐\n\n\n\nTesting Plan:\n# Evaluate on baseball clips\ntest_video = 'sac_fly_001.mp4'\n\nmetrics = {\n    'mAP': 'Mean Average Precision',\n    'FPS': 'Frames Per Second',\n    'IoU': 'Intersection over Union'\n}\n\n# Test each framework\nfor framework in [YOLO, CNN, TensorFlow]:\n    results = evaluate(framework, test_video)\n    print(f\"{framework}: {results}\")\nKey Findings:\n✅ YOLO: Top choice for real-time performance\n✅ TensorFlow: Better for post-game analysis\n✅ CNN: Best accuracy but slowest\nRecommendation: Use YOLO for player tracking",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1: Project Foundation and Technical Setup</span>"
    ]
  },
  {
    "objectID": "week1.html#technical-achievements",
    "href": "week1.html#technical-achievements",
    "title": "2  Week 1: Project Foundation and Technical Setup",
    "section": "2.4 Technical Achievements",
    "text": "2.4 Technical Achievements\n\n1. Development Environment\n\nOpenCV installed and tested\n\nVideo processing verified\n\nServer compatibility confirmed\n\nBaseline test script created\n\n\n\n2. Data Infrastructure\n\nGitHub repository established\n\nVideo samples collected\n\nFolder structure organized\n\nVersion control configured\n\n\n\n3. Data Source Validated\n\nMLB Film Room confirmed as primary source\n\nMetadata schema designed\n\nStatcast data availability verified\n\n25M+ datapoints per game identified\n\n\n\n4. Model Understanding\n\nExisting model analyzed\n\nFeature importance identified\n\nIntegration strategy designed\n\nChallenges documented\n\n\n\n5. Technology Selection\n\nComputer vision frameworks compared\n\nYOLO selected for tracking\n\nTesting plan established\n\nPerformance metrics defined",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1: Project Foundation and Technical Setup</span>"
    ]
  },
  {
    "objectID": "week1.html#key-decisions",
    "href": "week1.html#key-decisions",
    "title": "2  Week 1: Project Foundation and Technical Setup",
    "section": "2.5 Key Decisions",
    "text": "2.5 Key Decisions\n\nTechnology Stack\nObject Detection: YOLOv8\nMachine Learning: scikit-learn (Random Forest + Gradient Boosting + Logistic Regression)\nData Source: MLB Film Room\nCollaboration: GitHub\n\n\nFeature Strategy\nFocus on top 10 features (68.3% of predictive power) to: - Reduce complexity - Maintain performance - Enable video-based extraction\n\n\nDevelopment Approach\nPhase 1: Computer Vision (detect & track)\nPhase 2: Feature Extraction (calculate metrics)\nPhase 3: Model Integration (predictions)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1: Project Foundation and Technical Setup</span>"
    ]
  },
  {
    "objectID": "week1.html#challenges-identified",
    "href": "week1.html#challenges-identified",
    "title": "2  Week 1: Project Foundation and Technical Setup",
    "section": "2.6 Challenges Identified",
    "text": "2.6 Challenges Identified\n\nHardware to Software: Convert GPS-based features to video-based\nReal-time Processing: Maintain acceptable speed for tracking\nCamera Variance: Handle different angles and stadiums\nMissing Data: Deal with occluded players or missing overlays\nFeature Extraction: Calculate spatial metrics from 2D video",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1: Project Foundation and Technical Setup</span>"
    ]
  },
  {
    "objectID": "week1.html#next-steps-week-2",
    "href": "week1.html#next-steps-week-2",
    "title": "2  Week 1: Project Foundation and Technical Setup",
    "section": "2.7 Next Steps (Week 2)",
    "text": "2.7 Next Steps (Week 2)\n\nKeaton: Implement YOLOv8 player detection\nJoshua: Build frame extraction pipeline\nDuoduo: Develop spatial mapping (pixels → feet)\nDiego: Create feature documentation\nSamuel: Begin feature reduction testing",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1: Project Foundation and Technical Setup</span>"
    ]
  },
  {
    "objectID": "week1.html#success-metrics",
    "href": "week1.html#success-metrics",
    "title": "2  Week 1: Project Foundation and Technical Setup",
    "section": "2.8 Success Metrics",
    "text": "2.8 Success Metrics\n\n✅ Environment ready for development\n✅ Data source identified and validated\n✅ Model strategy documented\n✅ Technology decisions made\n✅ Team roles clearly defined",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1: Project Foundation and Technical Setup</span>"
    ]
  },
  {
    "objectID": "week1.html#lessons-learned",
    "href": "week1.html#lessons-learned",
    "title": "2  Week 1: Project Foundation and Technical Setup",
    "section": "2.9 Lessons Learned",
    "text": "2.9 Lessons Learned\n\nStart with Testing: Validating OpenCV early prevented later issues\nUnderstand the Baseline: Analyzing existing model informed our approach\nResearch Pays Off: Comparing frameworks saved time later\nClear Roles Help: Each member owning a component accelerated progress\nDocumentation Matters: Week 1 decisions guide all future work\n\n\nWeek 1 laid a solid foundation with clear technical direction, validated data sources, and a cohesive team strategy. Every subsequent week built directly on these initial decisions.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1: Project Foundation and Technical Setup</span>"
    ]
  }
]