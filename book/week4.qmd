# Week 4: Complete Pipeline and Performance Optimization

## Overview

Week 4 represented the culmination of our technical development, delivering a fully integrated pipeline that processes video end-to-end. The team focused on automation, optimization, robustness, and comprehensive system evaluation.

## Pipeline Goal

**Input**: Baseball video file
**Output**: SAFE or OUT prediction with probability

```
VIDEO → PLAYER TRACKING (YOLO) → FEATURE EXTRACTION → ML MODEL → PREDICTION
```

## Team Contributions

### Keaton Ruthardt: Complete Pipeline Integration

**Objective**: Build `run_complete_pipeline.py` as central entry point

**System Architecture**:

```python
"""
Complete pipeline: Video → Tracking → Features → Prediction
"""

def main():
    # Stage 1: Video Tracking
    tracker_csv = output_dir / f"{video_name}_tracker.csv"
    annotated_video = output_dir / f"{video_name}_annotated.mp4"
    run_tracker(args.video, str(tracker_csv), str(annotated_video), device = args.device)
    
    # Stage 2: Feature Extraction
    features_df = extract_features(str(tracker_csv), statcast_csv, video_name)

    # Stage 3: Save Features
    features_file = output_dir / f"{video_name}_features.csv"
    features_df.to_csv(features_files, index = False)
    
    # Stage 3: Model Prediction
    results = run_model_prediction(features_df)
    
    # Save output
    results_file = output_dir / f"{video_name}_prediction.txt"
    with open(results_file, 'w') as f:
        f.write(f"Video: {video_name}\n")
        f.write(f"Prediction: {'ADVANCES' if results['prediction'] == 1 else 'STAYS'}\n")
        f.write(f"Probability: {results['probability']:.3f}\n")
        f.write(f"\nFeatures:\n")
        for col in features_df.columns:
            if col != 'video_name':
                f.write(f"  {col}: {features_df[col].iloc[0]}\n")

    print(f"\n[OK] Results saved: {results_file}")
```

**Component Integration**:

1. **SimpleOutfielderTracker** (YOLOv8-based)
   - Detects players and ball
   - Tracks positions across frames
   - Exports tracker CSV

2. **BaseballFieldConverter**
   - Converts pixels → feet
   - Assigns fielder position labels (LF/CF/RF)
   - Merges with Statcast metadata

3. **RunnerAdvancePredictor**
   - Ensemble model (Random Forest + Gradient Boosting + Logistic Regression)
   - 7 final features
   - Probability output

**Automation Features**:

```python
# Automatic metadata loading
metadata = load_video_metadata('video_metadata.csv')
statcast_data = metadata.get_statcast_features(video_name)

# Intelligent frame windowing
tracking_window = (300, 450)  # Focus on outfield action

# 7 final model features extracted
features = [
    'runner_base',
    'exit_speed', 
    'exit_speed_squared',
    'launch_angle',
    'hit_distance',
    'hangtime',
    'fielder_pos'
]

# Complete output package
outputs = {
    'tracker_csv': f'{video_name}_tracker.csv',
    'features_csv': f'{video_name}_features.csv',
    'annotated_video': f'{video_name}_annotated.mp4',
    'prediction_txt': f'{video_name}_prediction.txt'
}
```

**Pipeline Outputs**:

```
results/
├── sac_fly_001_tracker.csv        # Frame-by-frame detections
├── sac_fly_001_features.csv       # 7 model-ready features
├── sac_fly_001_annotated.mp4      # Video with bounding boxes
└── sac_fly_001_prediction.txt     # SAFE/OUT + probability
```

**Example Prediction Output**:

```
Video: sac_fly_001
Prediction: RUNNER ADVANCES (SAFE)
Probability: 0.791881
Confidence: 79.2%
```

### Joshua Cano: Performance Benchmarking

**Objective**: Measure and optimize pipeline performance

**Testing Approach**:

```python
import time

def main():
    # Stage 1: Video Tracking
    t0 = time.perf_counter() # Added time variable
    tracker_csv = output_dir / f"{video_name}_tracker.csv"
    annotated_video = output_dir / f"{video_name}_annotated.mp4"
    run_tracker(args.video, str(tracker_csv), str(annotated_video), device = args.device)
    t1 = time.perf_counter() # Added time variable
    print(f"\n[Timing] Tracking stage took {t1 - t0:.1f} seconds") # Added to see how long this chunk of code took

    # Stage 2: Feature Extraction
    t2 = time.perf_counter() # Added time variable
    features_df = extract_features(str(tracker_csv), statcast_csv, video_name)
    t3 = time.perf_counter() # Added time variable
    print(f"\n[Timing] Feature extraction took {t3 - t2:.1f} seconds") # Added to see how long this chunk of code took

    # Stage 3: Save Features
    features_file = output_dir / f"{video_name}_features.csv"
    features_df.to_csv(features_files, index = False)
    
    # Stage 3: Model Prediction
    t4 = time.perf_counter() # Added time variable
    results = run_model_prediction(features_df)
    t5 = time.perf_counter() # Added time variable
    print(f"\n[Timing] Model training + prediction took {t5 - t4:.1f} seconds") # Added to see how long this chunk of code took

    # Save output
    results_file = output_dir / f"{video_name}_prediction.txt"
    with open(results_file, 'w') as f:
        f.write(f"Video: {video_name}\n")
        f.write(f"Prediction: {'ADVANCES' if results['prediction'] == 1 else 'STAYS'}\n")
        f.write(f"Probability: {results['probability']:.3f}\n")
        f.write(f"\nFeatures:\n")
        for col in features_df.columns:
            if col != 'video_name':
                f.write(f"  {col}: {features_df[col].iloc[0]}\n")

    print(f"\n[OK] Results saved: {results_file}")
```

**Performance Results**:

| Stage | CPU Time | MPS Time | Speedup |
|-------|----------|----------|---------|
| Tracking | 20.1s | 20.1s | 0% |
| Feature Extraction | 0.0s | 0.0s | - |
| Model Training + Prediction | 32.6s | 32.3s | 0.9% |
| **Total** | **52.7s** | **52.4s** | **0.6%** |

**Hardware Configuration Testing**:

**CPU Backend**:
```bash
python run_complete_pipeline.py \
  --video videos/sac_fly_001.mp4 \
  --metadata data/video_metadata.csv \
  --output results \
  --device cpu
```

**MPS Backend** (Apple Silicon GPU):
```bash
python run_complete_pipeline.py \
  --video videos/sac_fly_001.mp4 \
  --metadata data/video_metadata.csv \
  --output results \
  --device mps
```

**Analysis**:

**Why MPS didn't accelerate**:
1. YOLOv8n is very small → CPU keeps up easily
2. MPS backend still developing, has overhead
3. Short frame window (150 frames) limits GPU benefit
4. Most time spent in scikit-learn (CPU-only)

**Future Optimization**:
- CUDA deployment will show major speedups
- Larger YOLO models benefit more from GPU
- Batch processing multiple videos
- Pipeline now GPU-enabled for future scaling

### Samuel Bulnes: Model Improvements and Robustness

**Objective**: Stabilize model for production pipeline

**Key Changes**:

**1. Removed Unreliable Features**

```python
# REMOVED: Caused noise and crashes
removed_features = [
    'bearing',           # Inconsistent values
    'launch_direction'   # Noisy calculations
]

# KEPT: Stable, high-importance features
final_features = [
    'runner_base',       # 44.0% importance
    'hit_distance',      # 35.5%
    'exit_speed_squared',# 5.9%
    'exit_speed',        # 5.5%
    'hangtime',          # 3.1%
    'launch_angle',      # 2.9%
    'fielder_pos'        # 3.1%
]
```

**2. Robustness Improvements**

```python
def predict(self, features):
    """
    Handle edge cases gracefully
    """
    # Missing catch frames
    if 'catch_frame' not in features or pd.isna(features['catch_frame']):
        # Use last detection frame
        features['catch_frame'] = features['last_detection_frame']
    
    # Missing player detections
    if pd.isna(features['player_distance_ft']):
        # Impute with median
        features['player_distance_ft'] = self.median_distance
    
    # Unseen fielder positions
    if features['fielder_pos'] not in self.known_positions:
        # Map to closest known position
        features['fielder_pos'] = 'CF'  # Default
    
    return self.model.predict_proba(features)
```

**3. Pipeline Integration**

- Fully automatic inside tracking → features → prediction flow
- No manual preprocessing required
- Reproducible and stable across videos
- Survives missing data gracefully

**Performance Trade-off**:

| Metric | Old Model | New Model | Change |
|--------|-----------|-----------|--------|
| Log Loss | 0.37 | 0.40 | +8% |
| AUC | 0.91 | 0.89 | -2% |
| **Reliability** | **Medium** | **High** | **✅** |

**Justification**: Small performance decrease is acceptable for:
- ✅ Crash-free operation
- ✅ Consistent predictions
- ✅ Real-world compatibility
- ✅ Production readiness

### Duoduo Cai: System Evaluation

**Objective**: Comprehensive end-to-end testing on real videos

**Evaluation Process**:
def main():
    pred = load_predictions()
    labels = load_labels()
    df = pred.merge(labels, on="video_id", how="inner")
    if df.empty:
        raise RuntimeError(
            "Merged 0 rows. Check video_id alignment with metadata.")

    # Prepare arrays
    y_prob = df['predicted_prob'].clip(1e-9, 1 - 1e-9).to_numpy()
    y_true = df['actual_label'].astype(int).to_numpy()
    y_pred = (y_prob >= 0.5).astype(int)

    # Compute core metrics
    logloss = log_loss(y_true, y_prob)
    precision = precision_score(y_true, y_pred, zero_division=0)
    recall = recall_score(y_true, y_pred, zero_division=0)
    f1 = f1_score(y_true, y_pred, zero_division=0)

    # Compute counts
    correct = int((y_pred == y_true).sum())
    incorrect = int(len(y_true) - correct)
    fp = int(((y_pred == 1) & (y_true == 0)).sum())
    fn = int(((y_pred == 0) & (y_true == 1)).sum())

    # Display results in terminal
    print("\n=== Overall Metrics ===")
    print(f"Log Loss : {logloss:.3f}")
    print(f"Precision: {precision * 100:.1f}%")
    print(f"Recall   : {recall * 100:.1f}%")
    print(f"F1 Score : {f1 * 100:.1f}%")
    print(f"Samples  : {len(y_true)}")

    print("\nPrediction Distribution:")
    print(
        f"  - Correct  : {correct}/{len(y_true)} ({correct / len(y_true) * 100:.1f}%)")
    print(
        f"  - Incorrect: {incorrect}/{len(y_true)} ({incorrect / len(y_true) * 100:.1f}%)")
    print(f"  - False positives: {fp}")
    print(f"  - False negatives: {fn}")


**Test Dataset**: 35 sacrifice fly videos

**Results**:

**Valid Outputs**: 21 out of 35 videos (60%)

**Why 14 videos failed**:
- Empty tracker CSVs (no detections)
- Missing catch frames
- Invalid feature calculations
- Pipeline crashes

**Performance on Valid Outputs**:

- **All 21 were SAFE plays** (dataset bias)
- **Log Loss**: 0.181 (very low = confident predictions)
- **Prediction Distribution**:
  - Most SAFE probabilities: 0.85–0.95
  - Few borderline cases: 0.50–0.70
  - Model very confident on clean SAFE plays

**Key Finding**: 
OUT plays struggled due to tracking failures:
- Sharp camera angles
- Near infield frames
- Unpredictable player positions
- Harder for YOLO to detect reliably

**Recommendations**:
1. Improve tracking for OUT scenarios
2. Collect more OUT play videos
3. Handle edge cases better
4. Add fallback strategies for tracking failures

### Diego Mendoza: Video Overlay System

**Objective**: Design ESPN-style prediction graphics

**Research & Design**:

```python
/** 
* Display the video from webcam and overlay text 
*/ 
#include <stdio.h> 
#include "cv.h" 
#include "highgui.h" 
int main(int argc, char** argv) 
{ 
CvCapture *capture; 
IplImage *frame; 
double t, ms = 0; 
bool finished = false; 
/* initialize webcam */ 
capture = cvCaptureFromCAM(0); 
CvFont myFont; //Initialise font 
cvInitFont(&myFont,CV_FONT_HERSHEY_COMPLEX_SMALL ,1.5f,1.5f,0,1,8); while (!finished) 
{ 
/* display frame */ 
frame = cvQueryFrame(capture); 
cvPutText( frame, "My Text", cvPoint(50,50), &myFont , cvScalar(128) ); 
// use other functions to draw something cvShowImage("Stream",frame); } 
/* free memory */ 
cvReleaseCapture(&capture); 
return 0; 
}

```
## Overlay Code (Python in Quarto) 
```{r} 
library(reticulate) 
# If you want to force your ai-env: 
use_virtualenv("C:/ai-env", required = TRUE) 
``` 
```{python} 
import cv2 
from pathlib import Path 
# --- 1. Paths and prediction text ----------------------------------------- 
video_path = Path(r"C:\Users\Diego\Downloads\baseballVideos\sac_fly_001.mp4") output_path = video_path.with_name(video_path.stem + "_annotated.mp4") 
prediction_text = "Prediction: RUNNER ADVANCES (SAFE)" 
confidence_text = "Confidence: 79.2%" 
# --- 2. Open the video ---------------------------------------------------- 
cap = cv2.VideoCapture(str(video_path)) 
if not cap.isOpened(): 
raise RuntimeError(f"Could not open video: {video_path}") 
fps = cap.get(cv2.CAP_PROP_FPS) 
if fps == 0 or fps is None: 
fps = 30.0 
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) 
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) 
fourcc = cv2.VideoWriter_fourcc(*"mp4v") 
out = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height)) 
# --- 3. Configure font and overlay geometry -------------------------------- 
font = cv2.FONT_HERSHEY_SIMPLEX 
font_scale = 1.0 
thickness = 2 
text_color = (0, 0, 0) # black 
border_color = (0, 0, 0) # black

bg_color = (230, 230, 230) # light gray 
pred_pos = (40, 60) 
confidence_pos = (40, 110) 
(pred_size, pred_baseline) = cv2.getTextSize( 
prediction_text, font, font_scale, thickness 
) 
(conf_size, conf_baseline) = cv2.getTextSize( 
confidence_text, font, font_scale, thickness 
) 
pred_w, pred_h = pred_size 
conf_w, conf_h = conf_size 
padding = 20 
box_x1 = min(pred_pos[0], confidence_pos[0]) - padding 
box_x2 = max(pred_pos[0] + pred_w, confidence_pos[0] + conf_w) + padding box_y1 = pred_pos[1] - pred_h - padding 
box_y2 = confidence_pos[1] + conf_baseline + padding 
box_x1 = max(box_x1, 0) 
box_y1 = max(box_y1, 0) 
box_x2 = min(box_x2, width - 1) 
box_y2 = min(box_y2, height - 1) 
# --- 4. Main loop: read frames, draw box + text, write out ----------------- frame_count = 0 
while True: 
ret, frame = cap.read() 
if not ret: 
break 
cv2.rectangle( 
frame, 
(box_x1, box_y1), 
(box_x2, box_y2), 
bg_color, 
thickness=cv2.FILLED 
) 
cv2.rectangle( 
frame, 
(box_x1, box_y1), 
(box_x2, box_y2), 
border_color, 
thickness=2
) 
cv2.putText( 
frame, 
prediction_text, 
pred_pos, 
font, 
font_scale, 
text_color, 
thickness, 
cv2.LINE_AA 
) 
cv2.putText( 
frame, 
confidence_text, 
confidence_pos, 
font, 
font_scale, 
text_color, 
thickness, 
cv2.LINE_AA 
) 
out.write(frame) 
frame_count += 1 
# --- 5. Clean up ----------------------------------------------------------- 
cap.release() 
out.release() 
```


**Overlay Components**:
1. Prediction label ("SAFE" / "OUT")
2. Probability percentage
3. Color coding (green = SAFE, red = OUT)
4. Professional font and positioning

**Future Enhancements**:
- Player bounding boxes with IDs
- Distance measurements shown
- Real-time tracking visualization
- Animated graphics

## Technical Achievements

### 1. Complete Automation
✅ Single command runs entire pipeline
✅ Automatic metadata loading
✅ Intelligent frame windowing
✅ Error handling and logging

### 2. Performance Optimization
✅ Benchmarking infrastructure
✅ GPU support (MPS/CUDA ready)
✅ Timing instrumentation
✅ Scalability testing

### 3. Model Robustness
✅ Handles missing data
✅ Crash-free operation
✅ Consistent predictions
✅ Production-ready

### 4. System Validation
✅ 35-video test suite
✅ Quantitative metrics
✅ Failure analysis
✅ Actionable recommendations

## Pipeline Performance

**Processing Time per Video**:
- Tracking: ~20s
- Feature extraction: <1s
- Model prediction: ~33s
- **Total: ~53 seconds**

**Success Rate**:
- 60% of videos produce valid outputs
- Main failures: tracking detection issues
- Perfect performance on clean SAFE plays

**Model Confidence**:
- SAFE plays: 85-95% probability
- Very consistent predictions
- Low log loss (0.181) on valid outputs

## Challenges & Solutions

### Challenge 1: OUT Play Tracking
**Problem**: Many OUT plays failed tracking
**Solution**: Need better angle handling, more training data

### Challenge 2: GPU Acceleration
**Problem**: MPS didn't speed up pipeline
**Solution**: Identified bottlenecks, ready for CUDA

### Challenge 3: Dataset Bias
**Problem**: Test set only had SAFE plays pass validation
**Solution**: Prioritize OUT play collection for Week 5

### Challenge 4: Pipeline Crashes
**Problem**: Missing data caused failures
**Solution**: Added robust error handling and fallbacks

## Key Metrics

- **Pipeline Runtime**: 53 seconds per video
- **Success Rate**: 60% (21/35 videos)
- **Log Loss**: 0.181 (on valid outputs)
- **Prediction Confidence**: 85-95% for SAFE plays
- **Feature Count**: 7 final features

## Next Steps (Week 5)

1. Build final demo video with overlay graphics
2. Expand test dataset to ~120 videos
3. Focus on collecting OUT plays
4. Calculate final accuracy, precision, recall, F1
5. Complete documentation and GitHub book

---

*Week 4 delivered a production-ready pipeline with comprehensive testing and performance analysis.*
